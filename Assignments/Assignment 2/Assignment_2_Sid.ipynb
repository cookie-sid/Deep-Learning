{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAZ1qD7BWCVF"
      },
      "source": [
        "# Assignment 2: Build a CNN for image recognition.\n",
        "\n",
        "## Due Date:  March 29, 11:59PM\n",
        "\n",
        "### Name: Siddharth Harsukh Pansuria\n",
        "### CWID: 20005837\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-ihBZYWWCVG"
      },
      "source": [
        "## Introduction:\n",
        "\n",
        "1. In this assignment, you will build Convolutional Neural Network to classify CIFAR-10 Images.\n",
        "2. You can directly load dataset from many deep learning packages.\n",
        "3. You can use any deep learning packages such as pytorch, keras or tensorflow for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PibHnyC_WCVH"
      },
      "source": [
        "## Requirements:\n",
        "\n",
        "1. You need to load cifar 10 data and split the entire training dataset into training and validation.\n",
        "2. You will implement a CNN model to classify cifar 10 images with provided structure.\n",
        "3. You need to plot the training and validation accuracy or loss obtained from above step.\n",
        "4. Then you can use tuned parameters to train using the entire training dataset.\n",
        "5. You should report the testing accuracy using the model with complete data.\n",
        "6. You may try to change the structure (e.g, add BN layer or dropout layer,...) and analyze your findings.\n",
        "\n",
        "## Google Colab\n",
        "\n",
        "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvFm15mvWCVH"
      },
      "source": [
        "## Batch Normalization (BN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxgX9WkXWCVH"
      },
      "source": [
        "### Background:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69C2wFYoWCVI"
      },
      "source": [
        "- Batch Normalization is a technique to speed up training and help make the model more stable.\n",
        "- In simple words, batch normalization is just another network layer that gets inserted between a hidden layer and the next hidden layer. Its job is to take the outputs from the first hidden layer and normalize them before passing them on as the input of the next hidden layer.\n",
        "\n",
        "- For more detailed information, you may refer to the original paper: https://arxiv.org/pdf/1502.03167.pdf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKQeXItSWCVI"
      },
      "source": [
        "### BN Algorithm:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My9jWvdeWCVI"
      },
      "source": [
        "- Input: Values of $x$ over a mini-batch: $\\mathbf{B}$ = $\\{x_1,..., x_m\\};$\n",
        "- Output: $\\{y_i = BN_{\\gamma,\\beta}(x_i)\\}$, $\\gamma, \\beta$ are learnable parameters\n",
        "\n",
        "Normalization of the Input:\n",
        "$$\\mu_{\\mathbf{B}} = \\frac{1}{m}\\sum_{i=1}^m x_i$$\n",
        "$$\\sigma_{\\mathbf{B}}^2 = \\frac{1}{m}\\sum_{i=1}^m (x_i - \\mu_{\\mathbf{B}})^2$$\n",
        "$$\\hat{x_i} = \\frac{x_i - \\mu_{\\mathbf{B}}}{\\sqrt{\\sigma_{\\mathbf{B}}}^2 + \\epsilon}$$\n",
        "Re-scaling and Offsetting:\n",
        "$$y_i = \\gamma \\hat{x_i} + \\beta = BN_{\\gamma,\\beta}(x_i)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_t0_MMjWCVJ"
      },
      "source": [
        "### Advantages of BN:\n",
        "1. Improves gradient flow through the network.\n",
        "2. Allows use of saturating nonlinearities and higher learning rates.\n",
        "3. Makes weights easier to initialize.\n",
        "4. Act as a form of regularization and may reduce the need for dropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9PH3rbdWCVJ"
      },
      "source": [
        "### Implementation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iShLqN1WCVJ"
      },
      "source": [
        "- The batch normalization layer has already been implemented in many packages. You may simply call the function to build the layer. For example: torch.nn.BatchNorm2d() using pytroch package, keras.layers.BatchNormalization() using keras package.\n",
        "- The location of BN layer: Please make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x1NLMcLWCVJ"
      },
      "source": [
        "## 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2yigOUJWCVK"
      },
      "source": [
        "### 1.1. Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0wr42JmWCVK",
        "outputId": "3d3c0aa5-2951-4ad8-9177-ff5385179f63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 3s 0us/step\n",
            "170508288/170498071 [==============================] - 3s 0us/step\n",
            "shape of x_train: (50000, 32, 32, 3)\n",
            "shape of y_train: (50000, 1)\n",
            "shape of x_test: (10000, 32, 32, 3)\n",
            "shape of y_test: (10000, 1)\n",
            "number of classes: 10\n"
          ]
        }
      ],
      "source": [
        "# Load Cifar-10 Data\n",
        "# This is just an example, you may load dataset from other packages.\n",
        "import keras\n",
        "import numpy as np\n",
        "import tensorflow.keras\n",
        "\n",
        "### If you can not load keras dataset, un-comment these two lines.\n",
        "# import ssl\n",
        "# ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "print('shape of x_train: ' + str(x_train.shape))\n",
        "print('shape of y_train: ' + str(y_train.shape))\n",
        "print('shape of x_test: ' + str(x_test.shape))\n",
        "print('shape of y_test: ' + str(y_test.shape))\n",
        "print('number of classes: ' + str(np.max(y_train) - np.min(y_train) + 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvDDHZDIWCVL"
      },
      "source": [
        "### 1.2. One-hot encode the labels (5 points)\n",
        "\n",
        "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
        "\n",
        "1. Implement a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
        "\n",
        "2. Apply the function to ```y_train``` and ```y_test```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKA4wOBdWCVL",
        "outputId": "66df156a-f704-459d-e324-7bb32f785f4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of y_train_vec: (50000, 10)\n",
            "Shape of y_test_vec: (10000, 10)\n",
            "[6]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "def to_one_hot(y, num_class=10):\n",
        "    new_y = []\n",
        "    for val in y:\n",
        "        tempArr = np.zeros(num_class)\n",
        "        tempArr[val] = 1\n",
        "        new_y.append(tempArr)\n",
        "    \n",
        "    return np.asarray(new_y)\n",
        "    pass\n",
        "\n",
        "x_train, x_test = x_train.astype('float32') / 255, x_test.astype('float32') / 255\n",
        "\n",
        "y_train_vec = to_one_hot(y_train)\n",
        "y_test_vec = to_one_hot(y_test)\n",
        "\n",
        "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
        "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train_vec[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArjscD5AWCVL"
      },
      "source": [
        "#### Remark: the outputs should be\n",
        "* Shape of y_train_vec: (50000, 10)\n",
        "* Shape of y_test_vec: (10000, 10)\n",
        "* [6]\n",
        "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO3Cn_MFWCVL"
      },
      "source": [
        "### 1.3. Randomly partition the training set to training and validation sets (5 points)\n",
        "\n",
        "Randomly partition the 50K training samples to 2 sets: \n",
        "* a training set containing 40K samples: x_tr, y_tr\n",
        "* a validation set containing 10K samples: x_val, y_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jffNUbPWCVL",
        "outputId": "bb9d3796-db1c-4fa2-9284-5cfbcba1afaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x_tr: (40000, 32, 32, 3)\n",
            "Shape of y_tr: (40000, 10)\n",
            "Shape of x_val: (10000, 32, 32, 3)\n",
            "Shape of y_val: (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(x_train,y_train_vec,test_size=0.2,shuffle=True)\n",
        "\n",
        "print('Shape of x_tr: ' + str(x_tr.shape))\n",
        "print('Shape of y_tr: ' + str(y_tr.shape))\n",
        "print('Shape of x_val: ' + str(x_val.shape))\n",
        "print('Shape of y_val: ' + str(y_val.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdIhEl9dWCVL"
      },
      "source": [
        "## 2. Build a CNN and tune its hyper-parameters (50 points)\n",
        "\n",
        "- Build a convolutional neural network model using the below structure:\n",
        "\n",
        "- It should have a structure of: Conv - ReLU - Max Pool - ConV - ReLU - Max Pool - Dense - ReLU - Dense - Softmax\n",
        "\n",
        "- In the graph 3@32x32 means the dimension of input image, 32@30x30 means it has 32 filters and the dimension now becomes 30x30 after the convolution.\n",
        "- All convolutional layers (Conv) should have stride = 1 and no padding.\n",
        "- Max Pooling has a pool size of 2 by 2.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeXXZv5kWCVL"
      },
      "source": [
        "<img src=\"/content/network.PNG\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnfxyeV3WCVL"
      },
      "source": [
        "- You may use the validation data to tune the hyper-parameters (e.g., learning rate, and optimization algorithm)\n",
        "- Do NOT use test data for hyper-parameter tuning!!!\n",
        "- Try to achieve a validation accuracy as high as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgxzBKX7lC3S"
      },
      "source": [
        "## Original Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FNLG75QWCVM",
        "outputId": "e50bc7b2-9c3a-46b1-d599-74d037e2149a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 12, 12, 64)        32832     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2304)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               590080    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 626,378\n",
            "Trainable params: 626,378\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Build the model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (4, 4), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0sQuhHwYWCVM"
      },
      "outputs": [],
      "source": [
        "# Define model optimizer and loss function\n",
        "\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "lr = 0.0001\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(learning_rate = lr), metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWFGioHoWCVM",
        "outputId": "06f4a4b7-a909-49c0-c2a1-57c6d8fdca9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1000/1000 [==============================] - 22s 12ms/step - loss: 1.7798 - acc: 0.3679 - val_loss: 1.5964 - val_acc: 0.4207\n",
            "Epoch 2/100\n",
            "1000/1000 [==============================] - 15s 15ms/step - loss: 1.4843 - acc: 0.4716 - val_loss: 1.4098 - val_acc: 0.5029\n",
            "Epoch 3/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 1.3534 - acc: 0.5201 - val_loss: 1.3022 - val_acc: 0.5400\n",
            "Epoch 4/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 1.2680 - acc: 0.5525 - val_loss: 1.2445 - val_acc: 0.5591\n",
            "Epoch 5/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 1.1949 - acc: 0.5815 - val_loss: 1.1910 - val_acc: 0.5772\n",
            "Epoch 6/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 1.1371 - acc: 0.6043 - val_loss: 1.1490 - val_acc: 0.6010\n",
            "Epoch 7/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 1.0884 - acc: 0.6229 - val_loss: 1.1139 - val_acc: 0.6130\n",
            "Epoch 8/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 1.0437 - acc: 0.6373 - val_loss: 1.0750 - val_acc: 0.6331\n",
            "Epoch 9/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 1.0101 - acc: 0.6482 - val_loss: 1.0503 - val_acc: 0.6410\n",
            "Epoch 10/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.9759 - acc: 0.6611 - val_loss: 1.0392 - val_acc: 0.6344\n",
            "Epoch 11/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.9462 - acc: 0.6712 - val_loss: 1.0187 - val_acc: 0.6499\n",
            "Epoch 12/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.9203 - acc: 0.6815 - val_loss: 1.0384 - val_acc: 0.6424\n",
            "Epoch 13/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.8954 - acc: 0.6906 - val_loss: 0.9688 - val_acc: 0.6647\n",
            "Epoch 14/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.8712 - acc: 0.7007 - val_loss: 0.9662 - val_acc: 0.6693\n",
            "Epoch 15/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.8486 - acc: 0.7099 - val_loss: 0.9507 - val_acc: 0.6707\n",
            "Epoch 16/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.8267 - acc: 0.7146 - val_loss: 0.9251 - val_acc: 0.6805\n",
            "Epoch 17/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.8050 - acc: 0.7253 - val_loss: 0.9170 - val_acc: 0.6846\n",
            "Epoch 18/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.7857 - acc: 0.7322 - val_loss: 0.9608 - val_acc: 0.6771\n",
            "Epoch 19/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.7643 - acc: 0.7372 - val_loss: 0.9247 - val_acc: 0.6848\n",
            "Epoch 20/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.7449 - acc: 0.7458 - val_loss: 0.9355 - val_acc: 0.6828\n",
            "Epoch 21/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.7242 - acc: 0.7514 - val_loss: 0.9324 - val_acc: 0.6823\n",
            "Epoch 22/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.7066 - acc: 0.7595 - val_loss: 0.9521 - val_acc: 0.6767\n",
            "Epoch 23/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.6872 - acc: 0.7650 - val_loss: 0.9030 - val_acc: 0.6890\n",
            "Epoch 24/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.6694 - acc: 0.7728 - val_loss: 0.8959 - val_acc: 0.6983\n",
            "Epoch 25/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.6519 - acc: 0.7782 - val_loss: 0.8970 - val_acc: 0.6980\n",
            "Epoch 26/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.6330 - acc: 0.7860 - val_loss: 0.8803 - val_acc: 0.7030\n",
            "Epoch 27/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.6156 - acc: 0.7915 - val_loss: 0.8794 - val_acc: 0.7079\n",
            "Epoch 28/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.5974 - acc: 0.7965 - val_loss: 0.9306 - val_acc: 0.6942\n",
            "Epoch 29/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.5799 - acc: 0.8033 - val_loss: 0.9151 - val_acc: 0.6956\n",
            "Epoch 30/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.5642 - acc: 0.8094 - val_loss: 0.8796 - val_acc: 0.7067\n",
            "Epoch 31/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.5452 - acc: 0.8163 - val_loss: 0.8748 - val_acc: 0.7087\n",
            "Epoch 32/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.5278 - acc: 0.8220 - val_loss: 0.9149 - val_acc: 0.7003\n",
            "Epoch 33/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.5112 - acc: 0.8298 - val_loss: 0.8777 - val_acc: 0.7082\n",
            "Epoch 34/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4963 - acc: 0.8341 - val_loss: 0.9013 - val_acc: 0.7096\n",
            "Epoch 35/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4770 - acc: 0.8428 - val_loss: 0.8904 - val_acc: 0.7149\n",
            "Epoch 36/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4627 - acc: 0.8453 - val_loss: 0.8936 - val_acc: 0.7129\n",
            "Epoch 37/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4470 - acc: 0.8503 - val_loss: 0.9455 - val_acc: 0.7021\n",
            "Epoch 38/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4291 - acc: 0.8564 - val_loss: 0.9169 - val_acc: 0.7053\n",
            "Epoch 39/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4132 - acc: 0.8644 - val_loss: 0.9595 - val_acc: 0.7011\n",
            "Epoch 40/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.3993 - acc: 0.8683 - val_loss: 0.9219 - val_acc: 0.7134\n",
            "Epoch 41/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.3840 - acc: 0.8741 - val_loss: 0.9541 - val_acc: 0.7075\n",
            "Epoch 42/100\n",
            "1000/1000 [==============================] - 14s 14ms/step - loss: 0.3662 - acc: 0.8816 - val_loss: 0.9326 - val_acc: 0.7103\n",
            "Epoch 43/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3531 - acc: 0.8862 - val_loss: 0.9918 - val_acc: 0.6974\n",
            "Epoch 44/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.3385 - acc: 0.8921 - val_loss: 0.9741 - val_acc: 0.7045\n",
            "Epoch 45/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.3241 - acc: 0.8970 - val_loss: 1.0062 - val_acc: 0.7022\n",
            "Epoch 46/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.3094 - acc: 0.9016 - val_loss: 0.9814 - val_acc: 0.7061\n",
            "Epoch 47/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.2951 - acc: 0.9070 - val_loss: 1.0017 - val_acc: 0.7057\n",
            "Epoch 48/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.2816 - acc: 0.9118 - val_loss: 1.0234 - val_acc: 0.7011\n",
            "Epoch 49/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.2685 - acc: 0.9168 - val_loss: 1.0424 - val_acc: 0.7021\n",
            "Epoch 50/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.2560 - acc: 0.9210 - val_loss: 1.0350 - val_acc: 0.7083\n",
            "Epoch 51/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2417 - acc: 0.9272 - val_loss: 1.0987 - val_acc: 0.6978\n",
            "Epoch 52/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.2304 - acc: 0.9312 - val_loss: 1.0812 - val_acc: 0.7051\n",
            "Epoch 53/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.2175 - acc: 0.9357 - val_loss: 1.0906 - val_acc: 0.7098\n",
            "Epoch 54/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.2065 - acc: 0.9388 - val_loss: 1.1248 - val_acc: 0.7010\n",
            "Epoch 55/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1948 - acc: 0.9427 - val_loss: 1.1478 - val_acc: 0.7006\n",
            "Epoch 56/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1837 - acc: 0.9471 - val_loss: 1.1769 - val_acc: 0.6992\n",
            "Epoch 57/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1717 - acc: 0.9509 - val_loss: 1.1811 - val_acc: 0.6936\n",
            "Epoch 58/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1618 - acc: 0.9548 - val_loss: 1.1817 - val_acc: 0.7066\n",
            "Epoch 59/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1525 - acc: 0.9575 - val_loss: 1.2248 - val_acc: 0.7025\n",
            "Epoch 60/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1425 - acc: 0.9621 - val_loss: 1.2222 - val_acc: 0.7050\n",
            "Epoch 61/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1339 - acc: 0.9647 - val_loss: 1.2366 - val_acc: 0.7031\n",
            "Epoch 62/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1248 - acc: 0.9675 - val_loss: 1.2909 - val_acc: 0.6990\n",
            "Epoch 63/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1169 - acc: 0.9702 - val_loss: 1.3457 - val_acc: 0.6945\n",
            "Epoch 64/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1084 - acc: 0.9730 - val_loss: 1.3667 - val_acc: 0.6950\n",
            "Epoch 65/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.1005 - acc: 0.9752 - val_loss: 1.3960 - val_acc: 0.6960\n",
            "Epoch 66/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.0946 - acc: 0.9780 - val_loss: 1.4037 - val_acc: 0.6974\n",
            "Epoch 67/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.0869 - acc: 0.9794 - val_loss: 1.4133 - val_acc: 0.6966\n",
            "Epoch 68/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.0804 - acc: 0.9813 - val_loss: 1.4800 - val_acc: 0.6964\n",
            "Epoch 69/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.0741 - acc: 0.9833 - val_loss: 1.4686 - val_acc: 0.6985\n",
            "Epoch 70/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0682 - acc: 0.9851 - val_loss: 1.5455 - val_acc: 0.6916\n",
            "Epoch 71/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.0625 - acc: 0.9865 - val_loss: 1.5505 - val_acc: 0.6920\n",
            "Epoch 72/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.0578 - acc: 0.9884 - val_loss: 1.6171 - val_acc: 0.6915\n",
            "Epoch 73/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.0531 - acc: 0.9898 - val_loss: 1.5914 - val_acc: 0.7020\n",
            "Epoch 74/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.0490 - acc: 0.9911 - val_loss: 1.6094 - val_acc: 0.6959\n",
            "Epoch 75/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0446 - acc: 0.9918 - val_loss: 1.6617 - val_acc: 0.6941\n",
            "Epoch 76/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0414 - acc: 0.9930 - val_loss: 1.6609 - val_acc: 0.7011\n",
            "Epoch 77/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0375 - acc: 0.9937 - val_loss: 1.7273 - val_acc: 0.6984\n",
            "Epoch 78/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0349 - acc: 0.9939 - val_loss: 1.7250 - val_acc: 0.6951\n",
            "Epoch 79/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0322 - acc: 0.9948 - val_loss: 1.7836 - val_acc: 0.6971\n",
            "Epoch 80/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0287 - acc: 0.9956 - val_loss: 1.8263 - val_acc: 0.6963\n",
            "Epoch 81/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.0272 - acc: 0.9955 - val_loss: 1.8552 - val_acc: 0.6957\n",
            "Epoch 82/100\n",
            "1000/1000 [==============================] - 14s 14ms/step - loss: 0.0242 - acc: 0.9967 - val_loss: 1.9949 - val_acc: 0.6895\n",
            "Epoch 83/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0221 - acc: 0.9969 - val_loss: 1.9006 - val_acc: 0.6981\n",
            "Epoch 84/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.0204 - acc: 0.9973 - val_loss: 1.9399 - val_acc: 0.7001\n",
            "Epoch 85/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.0187 - acc: 0.9972 - val_loss: 2.0063 - val_acc: 0.6924\n",
            "Epoch 86/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.0180 - acc: 0.9970 - val_loss: 1.9900 - val_acc: 0.6956\n",
            "Epoch 87/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0169 - acc: 0.9975 - val_loss: 2.0649 - val_acc: 0.6929\n",
            "Epoch 88/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0153 - acc: 0.9978 - val_loss: 2.0747 - val_acc: 0.6917\n",
            "Epoch 89/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0144 - acc: 0.9979 - val_loss: 2.1325 - val_acc: 0.6899\n",
            "Epoch 90/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.0131 - acc: 0.9982 - val_loss: 2.1420 - val_acc: 0.6951\n",
            "Epoch 91/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0121 - acc: 0.9980 - val_loss: 2.3208 - val_acc: 0.6894\n",
            "Epoch 92/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.0111 - acc: 0.9985 - val_loss: 2.2090 - val_acc: 0.6932\n",
            "Epoch 93/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0103 - acc: 0.9984 - val_loss: 2.2136 - val_acc: 0.6981\n",
            "Epoch 94/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0100 - acc: 0.9983 - val_loss: 2.3630 - val_acc: 0.6880\n",
            "Epoch 95/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.0095 - acc: 0.9984 - val_loss: 2.3298 - val_acc: 0.6914\n",
            "Epoch 96/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.0085 - acc: 0.9987 - val_loss: 2.2914 - val_acc: 0.6952\n",
            "Epoch 97/100\n",
            "1000/1000 [==============================] - 10s 10ms/step - loss: 0.0081 - acc: 0.9987 - val_loss: 2.3401 - val_acc: 0.6952\n",
            "Epoch 98/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.0075 - acc: 0.9988 - val_loss: 2.3822 - val_acc: 0.6915\n",
            "Epoch 99/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.0071 - acc: 0.9988 - val_loss: 2.5083 - val_acc: 0.6872\n",
            "Epoch 100/100\n",
            "1000/1000 [==============================] - 11s 11ms/step - loss: 0.0068 - acc: 0.9988 - val_loss: 2.4315 - val_acc: 0.6975\n"
          ]
        }
      ],
      "source": [
        "# Train the model and store model parameters/loss values\n",
        "\n",
        "history_model_1 = model.fit(x_tr, y_tr, batch_size=40, epochs=100, validation_data=(x_val, y_val))\n",
        "model.save('model_1.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgI3M_dhWCVM"
      },
      "source": [
        "## 3. Plot the training and validation loss curve versus epochs. (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "J6ONkvzmWCVM",
        "outputId": "cd0eee02-d557-4a42-80b9-fb25f90ce92b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9b3/8deHsIZFZZFSAgFbFLXIFnHftRfFwnUHUaFYrSvK9afi1aqXSltbb931FvclLe5oW9Sqxap1I1pAUVHEEEJdEBTZIeTz++M7k0xCJpmETCaZ834+HvPInGXOfM+cyfcz3/WYuyMiItHVKtMJEBGRzFIgEBGJOAUCEZGIUyAQEYk4BQIRkYhrnekE1Ff37t29X79+mU6GiEiL8s4773zt7j1q2tbiAkG/fv0oKirKdDJERFoUM1uabJuqhkREIk6BQEQk4hQIREQirsW1EdRky5YtlJaWsnHjxkwnRZJo3749eXl5tGnTJtNJEZFqsiIQlJaW0rlzZ/r164eZZTo5Uo27s3LlSkpLS+nfv3+mkyMi1aStasjM7jWzr8zs/STbzcxuMbPFZrbAzIY19L02btxIt27dFASaKTOjW7duKrFJi1ZYCP36QatW4W9h4bbru3cPj4Y879cPzjuv7mMlvndjsXTNPmpmBwNrgQfd/Uc1bD8GuBA4BtgHuNnd96nruAUFBV69++iHH37I7rvv3ijplvTRdZLtVVgIV14JJSXQtWtYt2pVas/79oVjjoHZs+v/+pUrwQwSs8v4cvX1TSE3F2bMgPHjU3+Nmb3j7gU1bUtb1ZC7v2Jm/WrZZQwhSDjwppntaGa93P3zdKVJRDKvoZl59cx45crKY6byfOlSuPPO+r0m8Xn1zD6+nImZ/NevD59hfQJBbTLZa6g3sCxhuTS2bhtmdraZFZlZ0YoVK5okcfWxcuVKhgwZwpAhQ/je975H7969K5Y3b95c62uLioqYPHlyne+x//77N1ZyRZpETVUmZnD66SFTdg8Z7cqVqT2HzGS6zVVJSeMdq0V0H3X3Ge5e4O4FPXrUOEK6XpLV9TVUt27dmDdvHvPmzeOcc85hypQpFctt27alrKws6WsLCgq45ZZb6nyP119/ffsSKZIG1f+X4nXcyTJ8UGbeWPr2bbxjZTIQLAf6JCznxdalVWEhnH125Rd06dKw3NiNLxMnTuScc85hn3324bLLLuPtt99mv/32Y+jQoey///4sWrQIgJdffpljjz0WgGuvvZZJkyZx6KGHsssuu1QJEJ06darY/9BDD+XEE09k4MCBjB8/nng7z+zZsxk4cCDDhw9n8uTJFcdNVFxczEEHHcSwYcMYNmxYlQBz/fXXM2jQIAYPHszUqVMBWLx4MUceeSSDBw9m2LBhfPrpp437QUmLkMqv+3jVy9LYRAbK8NMnNxemT2/EA7p72h5AP+D9JNtGAc8CBuwLvJ3KMYcPH+7VffDBB9usSyY/3z18Ras+8vNTPkStrrnmGv/d737nEyZM8FGjRnlZWZm7u69evdq3bNni7u4vvPCCH3/88e7uPmfOHB81alTFa/fbbz/fuHGjr1ixwrt27eqbN292d/eOHTtW7N+lSxdftmyZb9261ffdd19/9dVXfcOGDZ6Xl+dLlixxd/exY8dWHDfRunXrfMOGDe7u/vHHH3v885w9e7bvt99+vm7dOnd3X7lypbu7jxgxwp988kl3d9+wYUPF9oaoz3WSzHj44fC/YOberVt4QFiu6f8mao/451D984gvxz+zxM8v1ef5+e7nnrvt51/Tfg8/XP9rCxR5knw1bY3FZvYn4FCgu5mVAtcAbWLB5/+A2YQeQ4uB9cBP05WWRMnq1Rqzvi3upJNOIicnB4DVq1czYcIEPvnkE8yMLVu21PiaUaNG0a5dO9q1a8fOO+/Ml19+SV5eXpV9RowYUbFuyJAhFBcX06lTJ3bZZZeKfvrjxo1jxowZ2xx/y5YtXHDBBcybN4+cnBw+/vhjAF588UV++tOfkpubC0DXrl1Zs2YNy5cv57jjjgPCoDDJPvHG26VLkzfGNqdf9/E0dusWlpui11D89dOnhwbaxAbvxPUtVTp7DY2rY7sD56fr/ZPp27ey6Fp9fWPr2LFjxfNf/OIXHHbYYTz11FMUFxdz6KGH1viadu3aVTzPycmpsX0hlX2SufHGG+nZsyfz58+nvLxcmXuE1NRbp3pPnKbK8LcnM28Ome748ZlPQ2NqEY3FjWn69FC/lqjR69tqsHr1anr3Dp2i7r///kY//m677caSJUsoLi4G4JFHHkmajl69etGqVSseeughtm7dCsBRRx3Ffffdx/r16wFYtWoVnTt3Ji8vj1mzZgGwadOmiu3SMsTr9jPVeBsf49mtW3iYQX4+PPRQeN+vvw6P8vLUnhcXZ1cG3FxELhCMHx8GYuTnV34p6zswoyEuu+wyrrjiCoYOHVqvX/Cp6tChA3fccQcjR45k+PDhdO7cmR122GGb/c477zweeOABBg8ezEcffVRRahk5ciSjR4+moKCAIUOGcMMNNwDw0EMPccstt7DXXnux//7788UXXzR62qVx1ZT5Q/oy/Hhmn58P555b9X+rpgxfmXnzk7aRxemikcXJrV27lk6dOuHunH/++QwYMIApU6ZkOlkVdJ3SJ1k9f2OqqTqnuVTVSN1qG1kcuRJBNrvrrrsYMmQIe+65J6tXr+bnP/95ppMkaZSuX/71qc7Rr/vskBWzj0owZcqUZlUCkMaX7Jd/Y2T+7iHD1y/86FEgEGnmGjPzV/WO1ESBQKSZqd7Nc80aiE9ZtT2Zv37tSzIKBCLNQCqDuupDmb/UhwKBSIbF57+KD9FoaH2/Mn9pKPUaagSHHXYYzz//fJV1N910E+eee27S1xx66KHEu8Eec8wxfPvtt9vsc+2111b0509m1qxZfPDBBxXLV199NS+++GJ9ki8ZEu/1c9pplUGgvhL78Md79agnj9SXAkEjGDduHDNnzqyybubMmYwbV+ssGxVmz57Njjvu2KD3rh4Ipk2bxpFHHtmgY0n6JevyWR/K/KWxKRA0ghNPPJG//vWvFTehKS4u5t///jcHHXQQ5557LgUFBey5555cc801Nb6+X79+fP311wBMnz6dXXfdlQMPPLBiqmoIYwT23ntvBg8ezAknnMD69et5/fXXeeaZZ7j00ksZMmQIn376KRMnTuTxxx8H4KWXXmLo0KEMGjSISZMmsWnTpor3u+aaaxg2bBiDBg3io48+2iZNmq668Wxvf/82bWruz6/MXxpL9rURXHwxzJvXuMccMgRuuinp5q5duzJixAieffZZxowZw8yZMzn55JMxM6ZPn07Xrl3ZunUrRxxxBAsWLGCvvfaq8TjvvPMOM2fOZN68eZSVlTFs2DCGDx8OwPHHH89ZZ50FwFVXXcU999zDhRdeyOjRozn22GM58cQTqxxr48aNTJw4kZdeeoldd92VM844gzvvvJOLL74YgO7du/Puu+9yxx13cMMNN3D33XdXef3OO+/MCy+8QPv27fnkk08YN24cRUVFPPvsszz99NO89dZb5ObmsmrVKgDGjx/P1KlTOe6449i4cSPl5eUN+6yzxPZ2+VR9vzQllQgaSWL1UGK10KOPPsqwYcMYOnQoCxcurFKNU92rr77KcccdR25uLl26dGH06NEV295//30OOuggBg0aRGFhIQsXLqw1PYsWLaJ///7suuuuAEyYMIFXXnmlYvvxxx8PwPDhwysmqku0ZcsWzjrrLAYNGsRJJ51Uke5Up6vOrT6zX4Qk3vwI6t/4q1/90tSyr0RQyy/3dBozZgxTpkzh3XffZf369QwfPpzPPvuMG264gblz57LTTjsxceJENm7c2KDjT5w4kVmzZjF48GDuv/9+Xn755e1Kb3wq62TTWGu66vpLLAU0RG5u00yAKFKdSgSNpFOnThx22GFMmjSpojTw3Xff0bFjR3bYYQe+/PJLnn322VqPcfDBBzNr1iw2bNjAmjVr+POf/1yxbc2aNfTq1YstW7ZQmHBfzc6dO7NmzZptjrXbbrtRXFzM4sWLgTCL6CGHHJLy+Wi66vqpXgpIVWLDr4KAZIoCQSMaN24c8+fPrwgEgwcPZujQoQwcOJBTTz2VAw44oNbXDxs2jFNOOYXBgwdz9NFHs/fee1ds++Uvf8k+++zDAQccwMCBAyvWjx07lt/97ncMHTq0SgNt+/btue+++zjppJMYNGgQrVq14pxzzkn5XDRddWoa0gVUvX6kudE01NJksuU6NWTKZzX+SqbVNg119rURiKRRQ0YBK/OX5k6BQCQFDWkIVuOvtBRZ00bQ0qq4oqYlXp/tGQWsxl9pSbKiRNC+fXtWrlxJt27dsHhLnDQb7s7KlStbVBfUhk4Ep1KAtERpDQRmNhK4GcgB7nb331Tbng/cC/QAVgGnuXtpfd8nLy+P0tJSVqxY0QiplnRo3749eXl5mU5GnRpSBaSGYGnp0hYIzCwHuB04CigF5prZM+6eOLT2BuBBd3/AzA4Hfg2cXt/3atOmDf3792+MZEsEbc+N35X5SzZIZ4lgBLDY3ZcAmNlMYAyQGAj2AP4r9nwOMCuN6RHZhqqARNLbWNwbWJawXBpbl2g+cHzs+XFAZzPrVv1AZna2mRWZWZGqf2R7xRuBW7WCCRMaNhBMQUCySaZ7Df0/4BAz+xdwCLAc2Fp9J3ef4e4F7l7Qo0ePpk6jZJHEqSDcYes237aaaRSwZLN0Vg0tB/okLOfF1lVw938TKxGYWSfgBHff9lZdItupoRPCqQpIoiCdJYK5wAAz629mbYGxwDOJO5hZdzOLp+EKQg8ikUbR0HEAqgKSqElbIHD3MuAC4HngQ+BRd19oZtPMLD7R/qHAIjP7GOgJTE9XeiRa6ntPgJwc3QFMoisrJp0TidNUECI1q23SuUw3Fos0mobcE0DVPyJZMsWERJtKASLbRyUCaZEa0hCsRmCRmqlEIC2O7gkg0rgUCKTFufLK1EcDqwpIpG6qGpIWI14dlGpbgKqARFKjEoG0CNWrg2qjUoBI/ahEIM1avBRw2mm1BwE1BIs0nEoE0uzU9/4AaggW2T4KBNIsJMv8UwkCxcVpT55IVlMgkIzbnpvDTNfsVCLbTW0EknH16Q4ap7YAkcajQCAZU9/uoBBKAQ8/rJlBRRqTAoE0KU0NIdL8qI1Amkx92gLiDcbqESSSfgoE0mRSbQtQ5i/StFQ1JGlXn7aAeHdQBQGRpqMSgaRFfQeFgbqDimSKSgTS6Opzv2A1BItknkoE0ujUFiDSsigQSKOIVwWVlKR+oxhNDSHSPCgQSIM1pB0A1BYg0tyktY3AzEaa2SIzW2xmU2vY3tfM5pjZv8xsgZkdk870SOOpTzsAqC1ApDlLWyAwsxzgduBoYA9gnJntUW23q4BH3X0oMBa4I13pkcaVajuAWcj8H3ooBAt1DRVpftJZIhgBLHb3Je6+GZgJjKm2jwNdYs93AP6dxvRII6jvmIDycmX+Is1dOtsIegPLEpZLgX2q7XMt8DczuxDoCByZxvRIA2lMgEh2y/Q4gnHA/e6eBxwDPGRm26TJzM42syIzK1qxYkWTJzLKNCZAJPulMxAsB/okLOfF1iU6E3gUwN3fANoD3asfyN1nuHuBuxf06NEjTcmVRKneKzhO7QAiLVc6q4bmAgPMrD8hAIwFTq22TwlwBHC/me1OCAT6yZ9h1WcJrYvGBIi0bGkrEbh7GXAB8DzwIaF30EIzm2Zmo2O7XQKcZWbzgT8BE91T7Y0uja2+pQBQW4BINkjrgDJ3nw3Mrrbu6oTnHwAHpDMNkpr6lAJ0rwCR7JLpxmLJMLUFiIimmIiw+pQCcnPVE0gkW6lEEGH1mSVUQUAkeykQRFCqo4Nzc+Hhh1UFJJLtFAgiIp75m8Hpp9cdBFQKEIkOtRFEQPW2gNo66KotQCR6VCKIALUFiEhtFAiyWH1nClVbgEg0qWooS9W3a6hGB4tEl0oEWSbVAWKaKVRE4lQiyCKplgI0NYSIJKozEJjZT4C/unt5E6RHGiDxxjF10UyhIlJdKlVDpwCfmNlvzWxguhMk9VP9xjG1UVuAiNSkzkDg7qcBQ4FPCfcNeCN2x7DOaU+dJNWQyeLUFiAiNUmpsdjdvwMeJ9yAvhdwHPBu7F7D0sTqWwrQNBEiUps6A4GZjTazp4CXgTbACHc/GhhMuLGMNIF4CaBVK5gwQaUAEWk8qfQaOgG40d1fSVzp7uvN7Mz0JEsSVe8NtHVr7ftrmggRqY9UqoauBd6OL5hZBzPrB+DuL6UlVVJFqlNEgEoBIlJ/qQSCx4DErqNbY+skzeozRYTaAkSkoVIJBK3dfXN8Ifa8bfqSJJBag3BOThghrFKAiGyPVNoIVpjZaHd/BsDMxgBfpzdZ0ZXq4LCsawdYtw7uuy/MkV1QAIMHh5NMlXtoPGmtwfIi9ZVKieAc4L/NrMTMlgGXAz9Pb7KiKdVuoVlVAigrCyczYABceCFMngz77w9duoTntd08Ic4dfvYz6NEDbr+9amu6ewgyIpJUnT+f3P1TYF8z6xRbXpv2VEVUKo3CGZsiorQUPv8c9t679v02boQFC6CoCJYtg/POgz59Krdv3QoPPgjvvAOLF8N778G//x0y/8ceC40iRUXw1FNw663QuXPdw6Fvuw3uvRd++EO44AK4557wvm+8Ac89B998A//4R91pbwh3+Mtf4HvfS378V1+FX/8avv4aHn8c+vZt/HSIbA93r/MBjAIuA66OP1J83UhgEbAYmFrD9huBebHHx8C3dR1z+PDhnm0eftg9P9895CrJH7m5Yd8mVVLift557m3burdq5f7KK8n3feIJ9w4dqia6Vy/3oqKw/Ztv3I8+Oqzv0sV9+HD3U05xnzXLvby86rHKy93POivse/PNYd0XX7j/4Q/ud93lvmZNWPfyy+45Oe6jR7tv3er+6KPuvXuH1+24o/vJJ7v36ePer5/7qlWVxy8rc3/rLfcVKyrXffddOP7hh7sfdJD7f/yH+4knut93n/umTdue72efVZ5Pbq77669X3f7aa+4HHhi29+gRzrl3b/f33kvlk6/09tvu/fu7n3mm+7vvVn4+773nPmOG+9NPuy9btu1nKJIAKPJkeXWyDRU7wP8BDwLLgGuA94B7UnhdDmFail0IjcvzgT1q2f9C4N66jpttgeDhh0MeUlcQyM9v4iBQWup+/vkhALRp43722e4/+IF7374hQ6/u1Vfd27VzHzHC/fHH3ZcuDRlV377hBG+91X3AgHCsO+9MLdPassX9P//T3cx9333D3/gHssMO7pMnhwx2t93cv/228nVr17r/61/h9e7ub74Z3vcnPwnvW1rqfthhlccaODC8T8eOYXn33cP2ESMqI3Renvvvfx8y3TvvdL/kknBeHTu6X399OLcdd3SfPz8EmV/+MgTOvDz3W25xX7cubPv+90Pan37a/eOP3ZcvD+lNZtWqkIZu3Sq/KEOGuO+887Zfkh493H/2s3DuzcGWLSGwfvJJ7ft99ZX7l182TZoibHsDwYJqfzsBr6bwuv2A5xOWrwCuqGX/14Gj6jputgWCukoCTVIKWLzY/ZFHQub03HPuF10UMvXWrUMAKC4O+731Vlh3yilVM/IPP3TfaaeQGSb+wnZ3//xz9733DifTs2cIGPWxYYP7yJHuP/qR+9VXuy9YEH55jxsX0tK5c3j/utx8c0jDGWe4d+0aMvAbb3T/zW/cjz02XIif/tT9jTeqnlt5ufuzz7ofckjVC2PmPmZMCHju4TPKywvnGA8yp54aShmJiotD4Kl+oXv2dD/gAPdzzgnXI/7eY8aEIPbmmyEA33ij+377uZ92mvu994Zg8s9/hkA7fnxliWz//cM1jQfDupSVua9fn3z7mjWh9PXUU2HfumzdGj5PCN+ladPcN26suk95eUh3+/Zhv6FD3adODSWprVtTS3cymze7P/lkON6RR7rvsUcoeTaFL7+svIaN6f33t6vUt72B4O3Y3zeB7wPtgMUpvO5E4O6E5dOB25Lsmw98DuQk2X42UAQU9e3bt8EfRHOSSnVQk5QCXn7ZvVOnqm+ckxOqIZYs2Xb/6dPDPrfd5v6Pf7jffXdI6M47u3/6ac3vsW6d+003heqLxvT556F6JhXl5e4nnBDSPmyY+6JF9X+/+fNDNU1pac0Z7IcfunfvHjK2u+9O/k+7enXIUB96KPxi/vWvw+d9yCEh8rdt637ZZaFUASHzT9WqVaHk8sMfhtf27x+u1bvvhuv1l7+433+/+69+FUp8xx4bSlRt2lT+8ujb133wYPeCghB0fvSjULqJfz8OPLD2z7283H3KlLDvJZeEHw4Q3uc3v3GfPdt94UL3UaPC+mOOCd+rQw4JwR1CFdrFF4fS1kknhfPo0sV9l13c99knvPbUU0O15U03hR8Mcd9+GzJ/CMcbNiwEAnD/7/8Ogay8PFzP3//e/a9/rb1UlmjBguTn/vnn4bzbtw+Pv/89+XFKStwffDBUO953n/vMmTWXtONeeCF8L/73f1NLZw22NxD8AtiRMNXEF7EMe1oKr6tPILgcuLWuY3qWlAhSqQ7Kz2+ChMyeHb6wu+8efnEWFYVfl/FfuTUpK9v213G3bu5z5zZBgrfT2rXhV3L1X6aNqaQk9eBUk+XL3SdOrPxsjzuuYb8Cy8rCL+J9903+JdtxR/e99goB8vLLQ3D4r/9yP/30UI02cqT7EUeETPfqq0OGec89oRTWubP7HXeEwPLIIyFTe+yxUKKcOjUcf/LkyrQ/+2xlZhx/tGsXqs0Sz2/1avc//jFU1bVrVxnMTjopHO/UU92POiqUHnbZJZTuwH3XXd3nzAk/NgYNCgFgxozKALFhQ6g2g/CZxANl/NG2bWgbuuiiEByeeML9gw8qSz/z54egCSFoXnppSGt5efjun3NO+F/KyXGfMMF9zz3DP3liCXjjxlBtevTRVQNr4ucxdqz7889XLRG99lo41qBB7itX1v+7ENPgQEDoXrp/wnI7YIfaXpOwb8pVQ8C/Et+ntkc2BIJmUR30xBPhCz10aKijrY8vvwz/ZM8/H0oNqVY/SOrmzg2Zc22/ElNRXh6C/JNPhl+Vb74Z6uzXrWv4MT/7rLIRPNljwoSaq3dWrQql0DvvDKWC2nz3nfvXX9ednuefD8Ei3nbUubP73/5W87533RWq4Y46KpTGli4N+15ySWh7qV46bt8+ZMBm4djXXVdZ5dWzZyg5QaiSmzSpsj3k889DcOrcOQTPSZNC4I2Xdq66qrJ08dlnoUry/PNDFWs8sN1+eyjFdekSlr/4ou7PohbbWyL4V137JHlda2AJ0D+hsXjPGvYbCBQDlspxW3IgSGt10Nq1qdXduoe6/rZtQ7E/sZFVJFVlZSHzeuutUHf9ySfhV/Nrr4Vfwal+FxvLunWhOm3o0JCOhiovD8GnqChUoU2Z4v7jH4egnPhr/O23Qwlin31CyaimgF1aGjpYQAgIZ5wRSka1fTYbNrgXFla2q0Ho8dYI1aq1BQIL25MzsxuAN4Anva6dt33tMcBNhB5E97r7dDObFktQfKTytUB7d5+ayjELCgq8qKioPsloFlK5n3DSMQLl5fDss2Gw1GuvwRVXwOWXhzmp3eHmm+HSS6F9exg2LIzM7dUr9MHv0gUOPxx69gzH+vJLGD4c2rQJ/fW7dUvH6YoIwBdfhDEzhx8OHTqk/jr3MA7mscfCQMtddtnupJjZO+5ekOT96vxlv4Yw6dxm4LvY8nd1vS5dj5ZaImhwddA//xnqQiH0yT/iiPB81KhQHz12bOXyBReE+s943Wr80blzaKRbsyb0j+/Qofl0MRSRJsH2VA01t0dLCwTbVR302muhzvIHPwgNcps3h6LrbbeF+v1WrcLjV7+qWh+7dWuoX12+PNQ1jx4d3ihe/1lY2ERnLyLNRW2BoM4pJszs4CQliVdqWi+Vtqs66J//hJEjQxXPyy/D979fue3888N0Bv/zP3DRRfDjH1d9batWoVqoc+fwuqefhhdfhKuugv/4Dzj11EY4OxHJFqm0Efw5YbE9MAJ4x90PT2fCkmlJbQR13Uugygyia9bAnDnwySdhDp6HHw6Z+Jw5VYOAiEgD1NZGkMqkcz+pdrA+hAZgSSKVqaTz88NcauOP+gquuiU0BH/7bdjYtSvstx/cf7+CgIikXUMmby8Fdm/shGSLelUHPfII5E+ETZvg+OPDzJl77RUCgYhIE0mljeBWIF5/1AoYArybzkS1ZHVNJZ2bG5tVuawsdAEdOBBmzoTddmuyNIqIJEqlRJBYIV8G/Mnd/5mm9LR4JSXJt1VUB40HHp8V6o5uuklBQEQyKpVA8Diw0d23AphZjpnlunsdt1CJlni7QLK29216B910Uxgk8pOf1PwCEZEmksqtKl8CEofEdQBeTE9yWqa6bjHZrcN6rv9Fwo3d5s4N3UMnTw53oBcRyaBUAkF7T7g9Zex5Pe4qnv1qaxfYpW8ZH33/ME6Z2h9eeCGsvPHGMPXDpElNl0gRkSRSCQTrzGxYfMHMhgMb0peklidZu4AZfHrxrXT/9G1o1y4M5rrkkjB/yJlnhgFfIiIZlkoguBh4zMxeNbPXgEeAC9KbrJahsDAMGou3C+zLGwzg44rt+3+/OIzmPfZYWLQITjsNfv/7MInc5MkZSbOISHWpDCiba2YDgXjXlkXuviW9yWr+EscLGOVcxXVM4xo20ZarmcadHS7hke7nwbcWBot17AgPPABHHQVr14YIIiLSDKQyxcT5QKG7fxtb3gkY5+53NEH6ttFcppiITx/RiTU8yBkcxywe4jRyWc8JPMl3PX9Ily8Xh95BF12U6eSKSMTVNsVEKlVDZ8WDAIC7fwOc1ViJa6lKSqAVW5nDYfyEP3MRN3EGD3ISj8Of/kSXLatgn33CaGERkWYslUCQY2YWXzCzHMIdxyIpsV3gZB6lgHeYyP3cwkWA0TffYOzYUFz4+9/VPVREmr1UBpQ9BzxiZn+ILf8ceDZ9SWq+qrcLXMl03mdP/kiY1rli+giATp0yl1ARkXpIJRBcDpwNnJQAHxMAAA77SURBVBNbXgB8L20pasYSxwscx1P8iIWM4484rapOHyEi0oKk0muo3MzeAn4AnAx0B55Id8Kao8rxAs5VXMciduVRTsYsyc1lRERagKSBwMx2BcbFHl8Txg/g7oc1TdKan759Q9X/sfyFocxjAvdTTg75fTOdMhGRhqutsfgj4HDgWHc/0N1vBbY2TbKap+nToVOHrVzNNJbQnz9yatV2ARGRFqi2QHA88Dkwx8zuMrMjAKtl/6wV7yl0+ukw3f+bvSniGqbRO79N5a0mRURaqKSBwN1nuftYYCAwhzDVxM5mdqeZ/TjZ6xKZ2UgzW2Rmi81sapJ9TjazD8xsoZn9sSEnkU6JM4uO80Imb/wtM1qfy8iHT6O4WEFARFq+OkcWV9k5jCo+CTjF3Y+oY98c4GPgKMLtLecSRiR/kLDPAOBR4HB3/8bMdnb3r2o7blOPLI6PIC5gLq9wMG+xD0fxAr3z26iBWERajO0dWVzB3b9x9xl1BYGYEcBid1/i7puBmcCYavucBdweG61MXUEgE0pKoCNreYrj+JKenMRjlNGm1juRiYi0JPUKBPXUG1iWsFwaW5doV2BXM/unmb1pZiNrOpCZnW1mRWZWtGLFijQlt2Z9+8Kp/JE8lnMGD/I1PSrWi4hkg3QGglS0BgYAhxK6qd5lZjtW3ylWCilw94IePXo0aQKnX+dcYHcwj8G8ykEA6ikkIlklnYFgOdAnYTkvti5RKfCMu29x988IbQoD0pimehv/gzfZy+fzSNfzMDPy81FPIRHJKukMBHOBAWbW38zaAmOBZ6rtM4tQGsDMuhOqipakMU0pi3cZfWj/O1hjnRny21MpL0c9hUQk66QtELh7GeFOZs8DHwKPuvtCM5tmZqNjuz0PrDSzDwhdVC9195XpSlOq4l1G1y79mpN5lPt9ApMmd6KwMNMpExFpfPXqPtocNEX30XiX0Uv5Lb/lcvbkfT5gT/LzNaeQiLRMjdZ9NCpKSsI00+fwf7zMIXzAnhXrRUSyjQJBDfr2hVN4hF34jNs5v8p6EZFso0BQg19fu4lf2ZXMYzBPcAKgLqMikr1SuTFN5Ixb/X/gn3HGzs/Bilbk99VNZ0QkeykQVLd6Nfzyl3DEETz4wo95MJLzrYpIlKhqqLrf/hZWroTrrwdTFBCR7KdAkODJ2/7Nhl/fyB8ZR78ThmvcgIhEgqqGYgoLYc2UX9Hat3AV17F0aRhUBmobEJHsphJBzB2XL2VS2QzuZRKfsQsA69fDlVdmOGEiImmmQBDz0+XX4RjXcVWV9RpEJiLZToEAYPFiJnIff+DnlFaZMFWDyEQk+ykQAEybBm3bcnOHK6qs1iAyEYkCBYKPP4bCQlpfdAHT7upFfn7oNar7DohIVKjX0FNPQXk5TJnC+F7K+EUkelQiePFFGDQIevXKdEpERDIi2oFgwwZ49VU48shMp0REJGMiHQhenPY6bNrEqBuPpF8/NJJYRCIpsoGgsBDm3fAiW2jNPzi4YiSxgoGIRE1kA8GVV8LBZS/xJvuyjk6ARhKLSDRFNhB8t/QbCijiRaq2D2gksYhETWQDwUk95tAK3yYQaCSxiERNZAPBJXu9yBo68TYjKtZpJLGIRFFaA4GZjTSzRWa22Mym1rB9opmtMLN5scfP0pmeRLuWvMh3Qw6hd34bjSQWkUhL28hiM8sBbgeOAkqBuWb2jLt/UG3XR9z9gnSlo0YlJfDJJ/S+8TyKL27SdxYRaXbSWSIYASx29yXuvhmYCYxJ4/ul7pVXwt/DD89sOkREmoF0BoLewLKE5dLYuupOMLMFZva4mfWpYXvj+/TTMLPcbrs1yduJiDRnmW4s/jPQz933Al4AHqhpJzM728yKzKxoxYoV2/+uS5fC974H7dpt/7FERFq4dAaC5VDlLi95sXUV3H2lu2+KLd4NDK/pQO4+w90L3L2gR48e25+ykpLQOiwiImkNBHOBAWbW38zaAmOBZxJ3MLPEKT9HAx+mMT2Vli7VgAERkZi0BQJ3LwMuAJ4nZPCPuvtCM5tmZqNju002s4VmNh+YDExMV3oqlJfDsmUqEYiIxKT1xjTuPhuYXW3d1QnPrwCuqP66tPrqK9i0SSUCEZGYTDcWN7nnZoTJhEZfmK+pp0VEiFggKCyEh6cvBaCYfE09LSJCxALBlVdCz82hRFBCqBrS1NMiEnWRCgQlJZDPUlbThdXsWGW9iEhURSoQ9O0LfSmpKA0krhcRiapIBYLp06G/LWUplV1HNfW0iERdpALB+PEwMHcp33Tqq6mnRURi0jqOoNlZs4Z2677h9N/kc/rlmU6MiEjzEKkSQUWrsBoFREQqRDMQaHoJEZEK0QoES8NgMpUIREQqRSsQlJRA69bQq1fd+4qIRES0AsHSpdCnD+TkZDolIiLNRrQCQUmJqoVERKqJViBYulQNxSIi1UQnEJSVwfLlKhGIiFQTnUCwfHm4O5lKBCIiVUQnEGgwmYhIjaITCOJjCFQiEBGpIjqBIF4i6NMns+kQEWlmohMILr4YPvoozDstIiIVohMIcnNht90ynQoRkWYnOoFARERqlNZAYGYjzWyRmS02s6m17HeCmbmZFaQzPSIisq20BQIzywFuB44G9gDGmdkeNezXGbgIeCtdaRERkeTSWSIYASx29yXuvhmYCYypYb9fAtcDG9OYFhERSSKdgaA3sCxhuTS2roKZDQP6uPtfazuQmZ1tZkVmVrRixYrGT6mISIRlrLHYzFoBvwcuqWtfd5/h7gXuXtCjR4/0J05EJELSGQiWA4mjt/Ji6+I6Az8CXjazYmBf4Bk1GIuINK10BoK5wAAz629mbYGxwDPxje6+2t27u3s/d+8HvAmMdveiNKZJRESqSVsgcPcy4ALgeeBD4FF3X2hm08xsdLreV0RE6qd1Og/u7rOB2dXWXZ1k30PTmRYREamZRhaLiERcJAJBYSH06wetWoW/hYWZTpGISPOR1qqh5qCwEM4+G9avD8tLl4ZlgPHjM5cuEZHmIutLBFdeWRkE4tavD+tFRCQCgSB+P5pU14uIRE3WB4JktyjWrYtFRIKsDwTTp297U7Lc3LBeREQiEAjGj4cZM8I9683C3xkz1FAsIhKX9b2GIGT6yvhFRGqW9SUCERGpnQKBiEjEKRCIiEScAoGISMQpEIiIRJy5e6bTUC9mtgJY2sCXdwe+bsTktBRRPO8onjNE87yjeM5Q//POd/ca7/Xb4gLB9jCzIneP3K0wo3jeUTxniOZ5R/GcoXHPW1VDIiIRp0AgIhJxUQsEMzKdgAyJ4nlH8ZwhmucdxXOGRjzvSLURiIjItqJWIhARkWoUCEREIi4ygcDMRprZIjNbbGZTM52edDCzPmY2x8w+MLOFZnZRbH1XM3vBzD6J/d0p02ltbGaWY2b/MrO/xJb7m9lbsev9iJm1zXQaG5uZ7Whmj5vZR2b2oZntF5FrPSX2/X7fzP5kZu2z7Xqb2b1m9pWZvZ+wrsZra8EtsXNfYGbD6vt+kQgEZpYD3A4cDewBjDOzPTKbqrQoAy5x9z2AfYHzY+c5FXjJ3QcAL8WWs81FwIcJy9cDN7r7D4FvgDMzkqr0uhl4zt0HAoMJ55/V19rMegOTgQJ3/xGQA4wl+673/cDIauuSXdujgQGxx9nAnfV9s0gEAmAEsNjdl7j7ZmAmMCbDaWp07v65u78be76GkDH0JpzrA7HdHgD+MzMpTA8zywNGAXfHlg04HHg8tks2nvMOwMHAPQDuvtndvyXLr3VMa6CDmbUGcoHPybLr7e6vAKuqrU52bccAD3rwJrCjmfWqz/tFJRD0BpYlLJfG1mUtM+sHDAXeAnq6++exTV8APTOUrHS5CbgMKI8tdwO+dfey2HI2Xu/+wArgvliV2N1m1pEsv9buvhy4ASghBIDVwDtk//WG5Nd2u/O3qASCSDGzTsATwMXu/l3iNg/9hbOmz7CZHQt85e7vZDotTaw1MAy4092HAuuoVg2UbdcaIFYvPoYQCL8PdGTbKpSs19jXNiqBYDnQJ2E5L7Yu65hZG0IQKHT3J2Orv4wXFWN/v8pU+tLgAGC0mRUTqvwOJ9Sd7xirOoDsvN6lQKm7vxVbfpwQGLL5WgMcCXzm7ivcfQvwJOE7kO3XG5Jf2+3O36ISCOYCA2I9C9oSGpeeyXCaGl2sbvwe4EN3/33CpmeACbHnE4Cnmzpt6eLuV7h7nrv3I1zXv7v7eGAOcGJst6w6ZwB3/wJYZma7xVYdAXxAFl/rmBJgXzPLjX3f4+ed1dc7Jtm1fQY4I9Z7aF9gdUIVUmrcPRIP4BjgY+BT4MpMpydN53ggobi4AJgXexxDqDN/CfgEeBHomum0pun8DwX+Enu+C/A2sBh4DGiX6fSl4XyHAEWx6z0L2CkK1xr4H+Aj4H3gIaBdtl1v4E+ENpAthNLfmcmuLWCEXpGfAu8RelTV6/00xYSISMRFpWpIRESSUCAQEYk4BQIRkYhTIBARiTgFAhGRiFMgEIkxs61mNi/h0WgTtplZv8SZJEWak9Z17yISGRvcfUimEyHS1FQiEKmDmRWb2W/N7D0ze9vMfhhb38/M/h6bA/4lM+sbW9/TzJ4ys/mxx/6xQ+WY2V2xufT/ZmYdYvtPjt1DYoGZzczQaUqEKRCIVOpQrWrolIRtq919EHAbYbZTgFuBB9x9L6AQuCW2/hbgH+4+mDD/z8LY+gHA7e6+J/AtcEJs/VRgaOw456Tr5ESS0chikRgzW+vunWpYXwwc7u5LYpP6feHu3czsa6CXu2+Jrf/c3bub2Qogz903JRyjH/CCh5uKYGaXA23c/Tozew5YS5gmYpa7r03zqYpUoRKBSGo8yfP62JTwfCuVbXSjCHPFDAPmJsyiKdIkFAhEUnNKwt83Ys9fJ8x4CjAeeDX2/CXgXKi4l/IOyQ5qZq2APu4+B7gc2AHYplQikk765SFSqYOZzUtYfs7d411IdzKzBYRf9eNi6y4k3CHsUsLdwn4aW38RMMPMziT88j+XMJNkTXKAh2PBwoBbPNxyUqTJqI1ApA6xNoICd/8602kRSQdVDYmIRJxKBCIiEacSgYhIxCkQiIhEnAKBiEjEKRCIiEScAoGISMT9fzOWThfHMsNOAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Plot the loss curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history_model_1.history['acc']\n",
        "val_acc = history_model_1.history['val_acc']\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYljmUOcWCVM"
      },
      "source": [
        "## 4. Train (again) and evaluate the model (5 points)\n",
        "\n",
        "- To this end, you have found the \"best\" hyper-parameters. \n",
        "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
        "- Evaluate your model on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W73oG3FWCVM"
      },
      "source": [
        "### Train the model on the entire training set\n",
        "\n",
        "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyPO7FtmWCVM"
      },
      "outputs": [],
      "source": [
        "#<Compile your model again (using the same hyper-parameters you tuned above)>\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation = 'relu', input_shape=(32, 32, 3)))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (4, 4), activation = 'relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation = 'relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(learning_rate = 0.0001) , metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cboJi14xWCVM",
        "outputId": "b86854bb-7841-44ac-fcca-54e33555616b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 1.7233 - acc: 0.3901\n",
            "Epoch 2/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 1.4056 - acc: 0.5018\n",
            "Epoch 3/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 1.2816 - acc: 0.5486\n",
            "Epoch 4/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 1.1984 - acc: 0.5807\n",
            "Epoch 5/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 1.1293 - acc: 0.6069\n",
            "Epoch 6/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 1.0736 - acc: 0.6251\n",
            "Epoch 7/100\n",
            "1250/1250 [==============================] - 13s 11ms/step - loss: 1.0270 - acc: 0.6424\n",
            "Epoch 8/100\n",
            "1250/1250 [==============================] - 12s 10ms/step - loss: 0.9852 - acc: 0.6585\n",
            "Epoch 9/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.9502 - acc: 0.6713\n",
            "Epoch 10/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.9191 - acc: 0.6832\n",
            "Epoch 11/100\n",
            "1250/1250 [==============================] - 12s 9ms/step - loss: 0.8902 - acc: 0.6922\n",
            "Epoch 12/100\n",
            "1250/1250 [==============================] - 13s 11ms/step - loss: 0.8620 - acc: 0.7030\n",
            "Epoch 13/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8362 - acc: 0.7105\n",
            "Epoch 14/100\n",
            "1250/1250 [==============================] - 12s 10ms/step - loss: 0.8119 - acc: 0.7210\n",
            "Epoch 15/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.7897 - acc: 0.7285\n",
            "Epoch 16/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.7665 - acc: 0.7369\n",
            "Epoch 17/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.7446 - acc: 0.7432\n",
            "Epoch 18/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.7209 - acc: 0.7535\n",
            "Epoch 19/100\n",
            "1250/1250 [==============================] - 12s 9ms/step - loss: 0.7019 - acc: 0.7576\n",
            "Epoch 20/100\n",
            "1250/1250 [==============================] - 12s 9ms/step - loss: 0.6818 - acc: 0.7660\n",
            "Epoch 21/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.6610 - acc: 0.7740\n",
            "Epoch 22/100\n",
            "1250/1250 [==============================] - 12s 10ms/step - loss: 0.6408 - acc: 0.7813\n",
            "Epoch 23/100\n",
            "1250/1250 [==============================] - 10s 8ms/step - loss: 0.6219 - acc: 0.7882\n",
            "Epoch 24/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.6022 - acc: 0.7930\n",
            "Epoch 25/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.5838 - acc: 0.8008\n",
            "Epoch 26/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.5664 - acc: 0.8073\n",
            "Epoch 27/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.5477 - acc: 0.8149\n",
            "Epoch 28/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.5309 - acc: 0.8205\n",
            "Epoch 29/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.5100 - acc: 0.8276\n",
            "Epoch 30/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4953 - acc: 0.8332\n",
            "Epoch 31/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4763 - acc: 0.8400\n",
            "Epoch 32/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4610 - acc: 0.8444\n",
            "Epoch 33/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.4431 - acc: 0.8511\n",
            "Epoch 34/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4265 - acc: 0.8579\n",
            "Epoch 35/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4117 - acc: 0.8624\n",
            "Epoch 36/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.3937 - acc: 0.8693\n",
            "Epoch 37/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.3795 - acc: 0.8731\n",
            "Epoch 38/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.3636 - acc: 0.8809\n",
            "Epoch 39/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.3472 - acc: 0.8874\n",
            "Epoch 40/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.3336 - acc: 0.8916\n",
            "Epoch 41/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.3192 - acc: 0.8964\n",
            "Epoch 42/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.3052 - acc: 0.9014\n",
            "Epoch 43/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.2910 - acc: 0.9068\n",
            "Epoch 44/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.2759 - acc: 0.9138\n",
            "Epoch 45/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.2622 - acc: 0.9163\n",
            "Epoch 46/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.2504 - acc: 0.9230\n",
            "Epoch 47/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.2377 - acc: 0.9261\n",
            "Epoch 48/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.2241 - acc: 0.9304\n",
            "Epoch 49/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.2108 - acc: 0.9363\n",
            "Epoch 50/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.2009 - acc: 0.9397\n",
            "Epoch 51/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1882 - acc: 0.9451\n",
            "Epoch 52/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1764 - acc: 0.9488\n",
            "Epoch 53/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1675 - acc: 0.9513\n",
            "Epoch 54/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1563 - acc: 0.9548\n",
            "Epoch 55/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1463 - acc: 0.9581\n",
            "Epoch 56/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1359 - acc: 0.9620\n",
            "Epoch 57/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1263 - acc: 0.9654\n",
            "Epoch 58/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1178 - acc: 0.9685\n",
            "Epoch 59/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1094 - acc: 0.9710\n",
            "Epoch 60/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.1023 - acc: 0.9739\n",
            "Epoch 61/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0943 - acc: 0.9768\n",
            "Epoch 62/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.0874 - acc: 0.9779\n",
            "Epoch 63/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0803 - acc: 0.9804\n",
            "Epoch 64/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0738 - acc: 0.9826\n",
            "Epoch 65/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0687 - acc: 0.9839\n",
            "Epoch 66/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0636 - acc: 0.9857\n",
            "Epoch 67/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0579 - acc: 0.9870\n",
            "Epoch 68/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0534 - acc: 0.9886\n",
            "Epoch 69/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0487 - acc: 0.9898\n",
            "Epoch 70/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0452 - acc: 0.9907\n",
            "Epoch 71/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.0403 - acc: 0.9924\n",
            "Epoch 72/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0384 - acc: 0.9922\n",
            "Epoch 73/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0340 - acc: 0.9940\n",
            "Epoch 74/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0315 - acc: 0.9943\n",
            "Epoch 75/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0295 - acc: 0.9942\n",
            "Epoch 76/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0273 - acc: 0.9946\n",
            "Epoch 77/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0247 - acc: 0.9958\n",
            "Epoch 78/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0242 - acc: 0.9954\n",
            "Epoch 79/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0219 - acc: 0.9958\n",
            "Epoch 80/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0200 - acc: 0.9963\n",
            "Epoch 81/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0184 - acc: 0.9967\n",
            "Epoch 82/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0182 - acc: 0.9963\n",
            "Epoch 83/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0158 - acc: 0.9975\n",
            "Epoch 84/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.0153 - acc: 0.9974\n",
            "Epoch 85/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0142 - acc: 0.9975\n",
            "Epoch 86/100\n",
            "1250/1250 [==============================] - 11s 8ms/step - loss: 0.0130 - acc: 0.9977\n",
            "Epoch 87/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0123 - acc: 0.9978\n",
            "Epoch 88/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0114 - acc: 0.9980\n",
            "Epoch 89/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0114 - acc: 0.9976\n",
            "Epoch 90/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0108 - acc: 0.9979\n",
            "Epoch 91/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0093 - acc: 0.9984\n",
            "Epoch 92/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0100 - acc: 0.9980\n",
            "Epoch 93/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0096 - acc: 0.9981\n",
            "Epoch 94/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0091 - acc: 0.9980\n",
            "Epoch 95/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0081 - acc: 0.9985\n",
            "Epoch 96/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0080 - acc: 0.9984\n",
            "Epoch 97/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0079 - acc: 0.9983\n",
            "Epoch 98/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0073 - acc: 0.9984\n",
            "Epoch 99/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0073 - acc: 0.9985\n",
            "Epoch 100/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 0.0066 - acc: 0.9988\n"
          ]
        }
      ],
      "source": [
        "#<Train your model on the entire training set (50K samples)>\n",
        "\n",
        "history_model_2 = model.fit(x_train, y_train_vec, batch_size=40, epochs=100)\n",
        "model.save('model_2.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxY36XYmWCVM"
      },
      "source": [
        "## 5. Evaluate the model on the test set (5 points)\n",
        "\n",
        "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzAdO2M1WCVM",
        "outputId": "eacb8507-e011-4892-d3f7-152c5b6a11d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 4ms/step - loss: 2.4996 - acc: 0.6904\n",
            "loss = 2.499619960784912\n",
            "accuracy = 0.6904000043869019\n"
          ]
        }
      ],
      "source": [
        "# Evaluate your model performance (testing accuracy) on testing data.\n",
        "from keras.models import load_model\n",
        "curr_model = load_model('model_2.h5')\n",
        "lacc = curr_model.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(lacc[0]))\n",
        "print('accuracy = ' + str(lacc[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yhfs40D7yo5M",
        "outputId": "f440d425-133b-42f5-c0f6-42f73d9bc18c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 4ms/step - loss: 2.5541 - acc: 0.7035\n",
            "loss = 2.5541446208953857\n",
            "accuracy = 0.703499972820282\n"
          ]
        }
      ],
      "source": [
        "# Evaluate your model performance (testing accuracy) on testing data.\n",
        "\n",
        "curr_model = load_model('model_1.h5')\n",
        "lacc = curr_model.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(lacc[0]))\n",
        "print('accuracy = ' + str(lacc[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUjql8w_WCVM"
      },
      "source": [
        "## 6. Building model with new structure (25 points)\n",
        "- In this section, you can build your model with adding new layers (e.g, BN layer or dropout layer, ...).\n",
        "- If you want to regularize a ```Conv/Dense layer```, you should place a ```Dropout layer``` before the ```Conv/Dense layer```.\n",
        "- You can try to compare their loss curve and testing accuracy and analyze your findings.\n",
        "- You need to try at lease two different model structures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8fUnkgMj-jb"
      },
      "source": [
        "## First Model using Batch Normalization + Data Augmentation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJtXZSyIWCVM",
        "outputId": "6b7ba5c7-d09d-4e61-d6e9-cc2ae0e915b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 30, 30, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 30, 30, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 12, 12, 64)        32832     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 12, 12, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 12, 12, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2304)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               590080    \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 627,786\n",
            "Trainable params: 627,082\n",
            "Non-trainable params: 704\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Build the model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (4, 4)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPZZb7V3O6Ln"
      },
      "outputs": [],
      "source": [
        "# Data augmentation\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "temp_data = ImageDataGenerator( rotation_range=20, height_shift_range=0.2, width_shift_range=0.2, \n",
        "                               zoom_range = 0.2, shear_range = 0.2, horizontal_flip=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrX-iVdvxYW9"
      },
      "outputs": [],
      "source": [
        "# Define model optimizer and loss function\n",
        "lr = 0.0001\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(learning_rate=lr), metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxgiRMjvP_zu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe4aadf-e49b-4b6c-d3bb-89688926aa56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "1000/1000 [==============================] - 48s 37ms/step - loss: 1.5709 - acc: 0.4343 - val_loss: 1.8509 - val_acc: 0.4142\n",
            "Epoch 2/150\n",
            "1000/1000 [==============================] - 39s 39ms/step - loss: 1.3131 - acc: 0.5324 - val_loss: 1.2476 - val_acc: 0.5543\n",
            "Epoch 3/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 1.2169 - acc: 0.5646 - val_loss: 1.1623 - val_acc: 0.5920\n",
            "Epoch 4/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 1.1501 - acc: 0.5927 - val_loss: 1.3540 - val_acc: 0.5460\n",
            "Epoch 5/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 1.1054 - acc: 0.6079 - val_loss: 1.0723 - val_acc: 0.6408\n",
            "Epoch 6/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 1.0624 - acc: 0.6260 - val_loss: 1.1799 - val_acc: 0.6115\n",
            "Epoch 7/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 1.0343 - acc: 0.6365 - val_loss: 0.8989 - val_acc: 0.6852\n",
            "Epoch 8/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 1.0105 - acc: 0.6436 - val_loss: 0.8509 - val_acc: 0.7093\n",
            "Epoch 9/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.9897 - acc: 0.6548 - val_loss: 1.0776 - val_acc: 0.6472\n",
            "Epoch 10/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.9684 - acc: 0.6599 - val_loss: 0.9053 - val_acc: 0.6832\n",
            "Epoch 11/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.9501 - acc: 0.6664 - val_loss: 0.9481 - val_acc: 0.6710\n",
            "Epoch 12/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.9333 - acc: 0.6744 - val_loss: 1.2209 - val_acc: 0.6078\n",
            "Epoch 13/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.9195 - acc: 0.6829 - val_loss: 1.0200 - val_acc: 0.6802\n",
            "Epoch 14/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.9174 - acc: 0.6793 - val_loss: 1.1082 - val_acc: 0.6363\n",
            "Epoch 15/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.9027 - acc: 0.6855 - val_loss: 1.3267 - val_acc: 0.5885\n",
            "Epoch 16/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.8905 - acc: 0.6904 - val_loss: 1.0966 - val_acc: 0.6663\n",
            "Epoch 17/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.8816 - acc: 0.6932 - val_loss: 0.9861 - val_acc: 0.6657\n",
            "Epoch 18/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.8758 - acc: 0.6939 - val_loss: 1.3534 - val_acc: 0.6073\n",
            "Epoch 19/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.8627 - acc: 0.6995 - val_loss: 1.0055 - val_acc: 0.6875\n",
            "Epoch 20/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.8605 - acc: 0.7024 - val_loss: 1.0549 - val_acc: 0.6507\n",
            "Epoch 21/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.8452 - acc: 0.7096 - val_loss: 1.1364 - val_acc: 0.6365\n",
            "Epoch 22/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.8405 - acc: 0.7070 - val_loss: 1.0697 - val_acc: 0.6779\n",
            "Epoch 23/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.8418 - acc: 0.7099 - val_loss: 0.9869 - val_acc: 0.6889\n",
            "Epoch 24/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.8312 - acc: 0.7114 - val_loss: 0.9559 - val_acc: 0.6853\n",
            "Epoch 25/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.8219 - acc: 0.7147 - val_loss: 1.0547 - val_acc: 0.6880\n",
            "Epoch 26/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.8179 - acc: 0.7177 - val_loss: 0.9026 - val_acc: 0.7048\n",
            "Epoch 27/150\n",
            "1000/1000 [==============================] - 36s 36ms/step - loss: 0.8172 - acc: 0.7191 - val_loss: 0.7884 - val_acc: 0.7393\n",
            "Epoch 28/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.8104 - acc: 0.7208 - val_loss: 0.8461 - val_acc: 0.7225\n",
            "Epoch 29/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.8104 - acc: 0.7178 - val_loss: 1.1209 - val_acc: 0.6668\n",
            "Epoch 30/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.8054 - acc: 0.7199 - val_loss: 0.8796 - val_acc: 0.7140\n",
            "Epoch 31/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.8068 - acc: 0.7220 - val_loss: 0.8961 - val_acc: 0.7158\n",
            "Epoch 32/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7984 - acc: 0.7262 - val_loss: 0.9977 - val_acc: 0.6674\n",
            "Epoch 33/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7949 - acc: 0.7237 - val_loss: 0.9091 - val_acc: 0.7041\n",
            "Epoch 34/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7864 - acc: 0.7284 - val_loss: 0.8487 - val_acc: 0.7154\n",
            "Epoch 35/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7813 - acc: 0.7307 - val_loss: 0.8580 - val_acc: 0.7278\n",
            "Epoch 36/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7770 - acc: 0.7296 - val_loss: 0.6804 - val_acc: 0.7716\n",
            "Epoch 37/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7795 - acc: 0.7316 - val_loss: 1.0658 - val_acc: 0.6854\n",
            "Epoch 38/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7786 - acc: 0.7332 - val_loss: 0.8386 - val_acc: 0.7299\n",
            "Epoch 39/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7730 - acc: 0.7315 - val_loss: 1.2945 - val_acc: 0.6053\n",
            "Epoch 40/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7659 - acc: 0.7363 - val_loss: 0.8963 - val_acc: 0.6922\n",
            "Epoch 41/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7723 - acc: 0.7331 - val_loss: 0.9063 - val_acc: 0.7304\n",
            "Epoch 42/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7690 - acc: 0.7342 - val_loss: 0.7438 - val_acc: 0.7565\n",
            "Epoch 43/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7664 - acc: 0.7329 - val_loss: 1.2383 - val_acc: 0.6622\n",
            "Epoch 44/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7668 - acc: 0.7359 - val_loss: 0.7783 - val_acc: 0.7464\n",
            "Epoch 45/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7538 - acc: 0.7385 - val_loss: 1.3705 - val_acc: 0.6544\n",
            "Epoch 46/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7557 - acc: 0.7401 - val_loss: 0.8783 - val_acc: 0.7293\n",
            "Epoch 47/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7458 - acc: 0.7428 - val_loss: 0.8034 - val_acc: 0.7249\n",
            "Epoch 48/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7541 - acc: 0.7409 - val_loss: 0.8449 - val_acc: 0.7348\n",
            "Epoch 49/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7434 - acc: 0.7422 - val_loss: 0.7680 - val_acc: 0.7434\n",
            "Epoch 50/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7467 - acc: 0.7431 - val_loss: 0.9461 - val_acc: 0.7010\n",
            "Epoch 51/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7416 - acc: 0.7434 - val_loss: 0.8288 - val_acc: 0.7214\n",
            "Epoch 52/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7392 - acc: 0.7418 - val_loss: 0.8111 - val_acc: 0.7343\n",
            "Epoch 53/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7439 - acc: 0.7425 - val_loss: 0.8905 - val_acc: 0.7148\n",
            "Epoch 54/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7365 - acc: 0.7469 - val_loss: 0.7350 - val_acc: 0.7701\n",
            "Epoch 55/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7330 - acc: 0.7473 - val_loss: 1.0014 - val_acc: 0.7048\n",
            "Epoch 56/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7329 - acc: 0.7487 - val_loss: 1.2969 - val_acc: 0.6751\n",
            "Epoch 57/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7316 - acc: 0.7504 - val_loss: 0.7412 - val_acc: 0.7623\n",
            "Epoch 58/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7297 - acc: 0.7477 - val_loss: 0.7698 - val_acc: 0.7594\n",
            "Epoch 59/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7304 - acc: 0.7484 - val_loss: 0.8635 - val_acc: 0.7363\n",
            "Epoch 60/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7246 - acc: 0.7515 - val_loss: 0.8163 - val_acc: 0.7564\n",
            "Epoch 61/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7196 - acc: 0.7514 - val_loss: 0.8938 - val_acc: 0.7367\n",
            "Epoch 62/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7223 - acc: 0.7532 - val_loss: 0.8325 - val_acc: 0.7616\n",
            "Epoch 63/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7192 - acc: 0.7508 - val_loss: 1.0009 - val_acc: 0.6876\n",
            "Epoch 64/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7177 - acc: 0.7524 - val_loss: 0.6450 - val_acc: 0.7828\n",
            "Epoch 65/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7170 - acc: 0.7545 - val_loss: 0.8553 - val_acc: 0.7256\n",
            "Epoch 66/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7246 - acc: 0.7510 - val_loss: 0.8161 - val_acc: 0.7473\n",
            "Epoch 67/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7066 - acc: 0.7543 - val_loss: 0.7394 - val_acc: 0.7599\n",
            "Epoch 68/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7118 - acc: 0.7571 - val_loss: 0.7887 - val_acc: 0.7468\n",
            "Epoch 69/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7113 - acc: 0.7558 - val_loss: 0.7773 - val_acc: 0.7469\n",
            "Epoch 70/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7097 - acc: 0.7562 - val_loss: 0.9089 - val_acc: 0.7287\n",
            "Epoch 71/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7087 - acc: 0.7584 - val_loss: 0.8094 - val_acc: 0.7447\n",
            "Epoch 72/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7137 - acc: 0.7574 - val_loss: 0.8997 - val_acc: 0.7450\n",
            "Epoch 73/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7034 - acc: 0.7578 - val_loss: 0.7110 - val_acc: 0.7682\n",
            "Epoch 74/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7042 - acc: 0.7580 - val_loss: 0.7658 - val_acc: 0.7523\n",
            "Epoch 75/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7050 - acc: 0.7588 - val_loss: 0.8603 - val_acc: 0.7352\n",
            "Epoch 76/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7017 - acc: 0.7587 - val_loss: 0.7804 - val_acc: 0.7535\n",
            "Epoch 77/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7044 - acc: 0.7603 - val_loss: 1.0086 - val_acc: 0.7194\n",
            "Epoch 78/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6993 - acc: 0.7630 - val_loss: 0.7748 - val_acc: 0.7406\n",
            "Epoch 79/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.7080 - acc: 0.7582 - val_loss: 1.6135 - val_acc: 0.6236\n",
            "Epoch 80/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6941 - acc: 0.7620 - val_loss: 0.8835 - val_acc: 0.7460\n",
            "Epoch 81/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.7050 - acc: 0.7577 - val_loss: 1.3045 - val_acc: 0.7037\n",
            "Epoch 82/150\n",
            "1000/1000 [==============================] - 38s 37ms/step - loss: 0.6947 - acc: 0.7617 - val_loss: 0.7149 - val_acc: 0.7540\n",
            "Epoch 83/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6884 - acc: 0.7621 - val_loss: 0.8599 - val_acc: 0.7626\n",
            "Epoch 84/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6886 - acc: 0.7636 - val_loss: 0.8450 - val_acc: 0.7436\n",
            "Epoch 85/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6968 - acc: 0.7620 - val_loss: 0.7156 - val_acc: 0.7668\n",
            "Epoch 86/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6952 - acc: 0.7602 - val_loss: 1.1763 - val_acc: 0.6778\n",
            "Epoch 87/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6858 - acc: 0.7627 - val_loss: 1.1435 - val_acc: 0.7138\n",
            "Epoch 88/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6909 - acc: 0.7626 - val_loss: 1.1655 - val_acc: 0.6941\n",
            "Epoch 89/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6839 - acc: 0.7651 - val_loss: 0.9327 - val_acc: 0.7469\n",
            "Epoch 90/150\n",
            "1000/1000 [==============================] - 38s 37ms/step - loss: 0.6870 - acc: 0.7635 - val_loss: 0.9299 - val_acc: 0.7246\n",
            "Epoch 91/150\n",
            "1000/1000 [==============================] - 38s 37ms/step - loss: 0.6826 - acc: 0.7660 - val_loss: 0.8273 - val_acc: 0.7466\n",
            "Epoch 92/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6828 - acc: 0.7663 - val_loss: 0.6899 - val_acc: 0.7819\n",
            "Epoch 93/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6813 - acc: 0.7677 - val_loss: 0.7036 - val_acc: 0.7854\n",
            "Epoch 94/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6819 - acc: 0.7658 - val_loss: 0.6773 - val_acc: 0.7838\n",
            "Epoch 95/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6848 - acc: 0.7667 - val_loss: 0.8086 - val_acc: 0.7302\n",
            "Epoch 96/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6815 - acc: 0.7631 - val_loss: 0.8065 - val_acc: 0.7536\n",
            "Epoch 97/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6779 - acc: 0.7706 - val_loss: 0.8495 - val_acc: 0.7363\n",
            "Epoch 98/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6732 - acc: 0.7693 - val_loss: 0.7528 - val_acc: 0.7704\n",
            "Epoch 99/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6756 - acc: 0.7685 - val_loss: 0.9652 - val_acc: 0.7480\n",
            "Epoch 100/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6825 - acc: 0.7653 - val_loss: 0.7591 - val_acc: 0.7702\n",
            "Epoch 101/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6734 - acc: 0.7697 - val_loss: 0.6473 - val_acc: 0.7975\n",
            "Epoch 102/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6738 - acc: 0.7688 - val_loss: 0.7032 - val_acc: 0.7719\n",
            "Epoch 103/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6678 - acc: 0.7686 - val_loss: 0.9652 - val_acc: 0.7350\n",
            "Epoch 104/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6659 - acc: 0.7699 - val_loss: 1.1305 - val_acc: 0.7195\n",
            "Epoch 105/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6637 - acc: 0.7728 - val_loss: 0.7296 - val_acc: 0.7600\n",
            "Epoch 106/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6743 - acc: 0.7705 - val_loss: 0.7666 - val_acc: 0.7518\n",
            "Epoch 107/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6667 - acc: 0.7685 - val_loss: 0.8754 - val_acc: 0.7548\n",
            "Epoch 108/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6612 - acc: 0.7731 - val_loss: 0.9168 - val_acc: 0.7448\n",
            "Epoch 109/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6665 - acc: 0.7692 - val_loss: 0.6695 - val_acc: 0.7800\n",
            "Epoch 110/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6706 - acc: 0.7695 - val_loss: 0.7223 - val_acc: 0.7770\n",
            "Epoch 111/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6691 - acc: 0.7704 - val_loss: 0.7592 - val_acc: 0.7745\n",
            "Epoch 112/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6680 - acc: 0.7689 - val_loss: 0.7449 - val_acc: 0.7536\n",
            "Epoch 113/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6596 - acc: 0.7753 - val_loss: 0.7438 - val_acc: 0.7596\n",
            "Epoch 114/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6669 - acc: 0.7722 - val_loss: 0.9477 - val_acc: 0.7503\n",
            "Epoch 115/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6589 - acc: 0.7721 - val_loss: 0.8235 - val_acc: 0.7401\n",
            "Epoch 116/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6549 - acc: 0.7762 - val_loss: 0.7333 - val_acc: 0.7567\n",
            "Epoch 117/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6608 - acc: 0.7729 - val_loss: 0.6988 - val_acc: 0.7814\n",
            "Epoch 118/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6583 - acc: 0.7738 - val_loss: 0.7014 - val_acc: 0.7719\n",
            "Epoch 119/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6567 - acc: 0.7759 - val_loss: 0.7677 - val_acc: 0.7476\n",
            "Epoch 120/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6582 - acc: 0.7732 - val_loss: 0.6994 - val_acc: 0.7820\n",
            "Epoch 121/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6586 - acc: 0.7736 - val_loss: 0.6459 - val_acc: 0.7909\n",
            "Epoch 122/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6508 - acc: 0.7751 - val_loss: 0.9530 - val_acc: 0.7088\n",
            "Epoch 123/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6592 - acc: 0.7738 - val_loss: 0.7994 - val_acc: 0.7554\n",
            "Epoch 124/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6571 - acc: 0.7732 - val_loss: 0.7958 - val_acc: 0.7583\n",
            "Epoch 125/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6487 - acc: 0.7773 - val_loss: 0.7261 - val_acc: 0.7811\n",
            "Epoch 126/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6517 - acc: 0.7778 - val_loss: 0.8815 - val_acc: 0.7010\n",
            "Epoch 127/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6564 - acc: 0.7753 - val_loss: 0.7075 - val_acc: 0.7697\n",
            "Epoch 128/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6579 - acc: 0.7757 - val_loss: 0.8943 - val_acc: 0.7196\n",
            "Epoch 129/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6516 - acc: 0.7760 - val_loss: 1.1567 - val_acc: 0.7409\n",
            "Epoch 130/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6510 - acc: 0.7783 - val_loss: 0.9322 - val_acc: 0.7497\n",
            "Epoch 131/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6505 - acc: 0.7754 - val_loss: 0.8017 - val_acc: 0.7522\n",
            "Epoch 132/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6484 - acc: 0.7763 - val_loss: 0.9720 - val_acc: 0.7069\n",
            "Epoch 133/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6431 - acc: 0.7792 - val_loss: 1.0560 - val_acc: 0.7589\n",
            "Epoch 134/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6446 - acc: 0.7766 - val_loss: 0.8288 - val_acc: 0.7451\n",
            "Epoch 135/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6345 - acc: 0.7808 - val_loss: 0.9101 - val_acc: 0.7207\n",
            "Epoch 136/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6475 - acc: 0.7783 - val_loss: 0.6892 - val_acc: 0.7863\n",
            "Epoch 137/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6459 - acc: 0.7792 - val_loss: 0.7772 - val_acc: 0.7791\n",
            "Epoch 138/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6463 - acc: 0.7787 - val_loss: 0.7465 - val_acc: 0.7630\n",
            "Epoch 139/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6446 - acc: 0.7789 - val_loss: 0.8365 - val_acc: 0.7570\n",
            "Epoch 140/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6465 - acc: 0.7771 - val_loss: 0.9853 - val_acc: 0.7548\n",
            "Epoch 141/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6445 - acc: 0.7791 - val_loss: 1.0550 - val_acc: 0.7587\n",
            "Epoch 142/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6370 - acc: 0.7797 - val_loss: 0.7548 - val_acc: 0.7586\n",
            "Epoch 143/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6473 - acc: 0.7809 - val_loss: 0.8139 - val_acc: 0.7540\n",
            "Epoch 144/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6440 - acc: 0.7788 - val_loss: 0.8045 - val_acc: 0.7690\n",
            "Epoch 145/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6429 - acc: 0.7800 - val_loss: 0.8856 - val_acc: 0.7220\n",
            "Epoch 146/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6388 - acc: 0.7821 - val_loss: 0.6685 - val_acc: 0.7850\n",
            "Epoch 147/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6412 - acc: 0.7789 - val_loss: 0.7627 - val_acc: 0.7653\n",
            "Epoch 148/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6378 - acc: 0.7807 - val_loss: 0.7645 - val_acc: 0.7446\n",
            "Epoch 149/150\n",
            "1000/1000 [==============================] - 38s 38ms/step - loss: 0.6400 - acc: 0.7806 - val_loss: 0.9536 - val_acc: 0.7592\n",
            "Epoch 150/150\n",
            "1000/1000 [==============================] - 37s 37ms/step - loss: 0.6275 - acc: 0.7861 - val_loss: 1.0783 - val_acc: 0.6692\n"
          ]
        }
      ],
      "source": [
        "# Fits the model on batches with real-time data augmentation\n",
        "\n",
        "history_model_3 = model.fit(temp_data.flow(x_tr, y_tr, batch_size=40), steps_per_epoch=x_tr.shape[0] // 40, epochs=150, validation_data=(x_val, y_val), validation_batch_size=x_val.shape[0] // 40)\n",
        "model.save('model_3.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzFyrg0MlfRe"
      },
      "source": [
        "## Plot the training and validation loss curve versus epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6iGRFYJxf-O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "71307120-2874-4bee-9616-18053e35bb92"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Plot the loss curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history_model_3.history['acc']\n",
        "val_acc = history_model_3.history['val_acc']\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FODq4jJkmI3s"
      },
      "source": [
        "## Train (again) and evaluate the model\n",
        "- To this end, you have found the \"best\" hyper-parameters. \n",
        "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
        "- Evaluate your model on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbskoYCFmWMj"
      },
      "source": [
        "### Train the model on the entire training set\n",
        "\n",
        "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ss--sTammX0-"
      },
      "outputs": [],
      "source": [
        "#<Compile your model again (using the same hyper-parameters you tuned above)>\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (4, 4)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(learning_rate = lr) , metrics=['acc'])\n",
        "\n",
        "# Data augmentation\n",
        "\n",
        "temp_data = ImageDataGenerator( rotation_range=20, height_shift_range=0.2, width_shift_range=0.2, \n",
        "                               zoom_range = 0.2, shear_range = 0.2, horizontal_flip=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiHQcf8AnW32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1451ae6f-0ee8-4770-e27f-3397be5dc49b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.6466 - acc: 0.4034\n",
            "Epoch 2/150\n",
            "1250/1250 [==============================] - 37s 30ms/step - loss: 1.4427 - acc: 0.4816\n",
            "Epoch 3/150\n",
            "1250/1250 [==============================] - 37s 30ms/step - loss: 1.3547 - acc: 0.5174\n",
            "Epoch 4/150\n",
            "1250/1250 [==============================] - 37s 30ms/step - loss: 1.2874 - acc: 0.5434\n",
            "Epoch 5/150\n",
            "1250/1250 [==============================] - 37s 30ms/step - loss: 1.2400 - acc: 0.5596\n",
            "Epoch 6/150\n",
            "1250/1250 [==============================] - 37s 30ms/step - loss: 1.2068 - acc: 0.5718\n",
            "Epoch 7/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.1824 - acc: 0.5831\n",
            "Epoch 8/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.1539 - acc: 0.5899\n",
            "Epoch 9/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.1277 - acc: 0.6019\n",
            "Epoch 10/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.1102 - acc: 0.6067\n",
            "Epoch 11/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.0965 - acc: 0.6154\n",
            "Epoch 12/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.0785 - acc: 0.6222\n",
            "Epoch 13/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.0686 - acc: 0.6249\n",
            "Epoch 14/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.0502 - acc: 0.6309\n",
            "Epoch 15/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.0401 - acc: 0.6355\n",
            "Epoch 16/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.0250 - acc: 0.6421\n",
            "Epoch 17/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.0113 - acc: 0.6444\n",
            "Epoch 18/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.0046 - acc: 0.6482\n",
            "Epoch 19/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.9917 - acc: 0.6523\n",
            "Epoch 20/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.9800 - acc: 0.6562\n",
            "Epoch 21/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.9669 - acc: 0.6589\n",
            "Epoch 22/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.9572 - acc: 0.6652\n",
            "Epoch 23/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.9545 - acc: 0.6644\n",
            "Epoch 24/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.9429 - acc: 0.6698\n",
            "Epoch 25/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.9321 - acc: 0.6720\n",
            "Epoch 26/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.9286 - acc: 0.6749\n",
            "Epoch 27/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.9225 - acc: 0.6799\n",
            "Epoch 28/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.9145 - acc: 0.6784\n",
            "Epoch 29/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.9060 - acc: 0.6870\n",
            "Epoch 30/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.8989 - acc: 0.6855\n",
            "Epoch 31/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8938 - acc: 0.6893\n",
            "Epoch 32/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8897 - acc: 0.6864\n",
            "Epoch 33/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8822 - acc: 0.6900\n",
            "Epoch 34/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8799 - acc: 0.6918\n",
            "Epoch 35/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8751 - acc: 0.6957\n",
            "Epoch 36/150\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 0.8687 - acc: 0.6965\n",
            "Epoch 37/150\n",
            "1250/1250 [==============================] - 41s 32ms/step - loss: 0.8640 - acc: 0.6983\n",
            "Epoch 38/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.8563 - acc: 0.7011\n",
            "Epoch 39/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8562 - acc: 0.7014\n",
            "Epoch 40/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8524 - acc: 0.7049\n",
            "Epoch 41/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8491 - acc: 0.7060\n",
            "Epoch 42/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8405 - acc: 0.7081\n",
            "Epoch 43/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8337 - acc: 0.7093\n",
            "Epoch 44/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.8357 - acc: 0.7094\n",
            "Epoch 45/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8291 - acc: 0.7124\n",
            "Epoch 46/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8219 - acc: 0.7138\n",
            "Epoch 47/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8218 - acc: 0.7148\n",
            "Epoch 48/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.8219 - acc: 0.7160\n",
            "Epoch 49/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.8187 - acc: 0.7148\n",
            "Epoch 50/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.8121 - acc: 0.7172\n",
            "Epoch 51/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.8125 - acc: 0.7183\n",
            "Epoch 52/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.8080 - acc: 0.7181\n",
            "Epoch 53/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.8007 - acc: 0.7219\n",
            "Epoch 54/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.8038 - acc: 0.7192\n",
            "Epoch 55/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.7943 - acc: 0.7238\n",
            "Epoch 56/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7973 - acc: 0.7236\n",
            "Epoch 57/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7917 - acc: 0.7206\n",
            "Epoch 58/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7864 - acc: 0.7242\n",
            "Epoch 59/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7879 - acc: 0.7251\n",
            "Epoch 60/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7862 - acc: 0.7262\n",
            "Epoch 61/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.7857 - acc: 0.7266\n",
            "Epoch 62/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7811 - acc: 0.7271\n",
            "Epoch 63/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7771 - acc: 0.7293\n",
            "Epoch 64/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7772 - acc: 0.7295\n",
            "Epoch 65/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7736 - acc: 0.7312\n",
            "Epoch 66/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7646 - acc: 0.7342\n",
            "Epoch 67/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7699 - acc: 0.7321\n",
            "Epoch 68/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7604 - acc: 0.7359\n",
            "Epoch 69/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7650 - acc: 0.7339\n",
            "Epoch 70/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7663 - acc: 0.7321\n",
            "Epoch 71/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7583 - acc: 0.7373\n",
            "Epoch 72/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7565 - acc: 0.7354\n",
            "Epoch 73/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7546 - acc: 0.7366\n",
            "Epoch 74/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7555 - acc: 0.7382\n",
            "Epoch 75/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7501 - acc: 0.7392\n",
            "Epoch 76/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7453 - acc: 0.7436\n",
            "Epoch 77/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7431 - acc: 0.7406\n",
            "Epoch 78/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7467 - acc: 0.7408\n",
            "Epoch 79/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7392 - acc: 0.7429\n",
            "Epoch 80/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7410 - acc: 0.7433\n",
            "Epoch 81/150\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.7445 - acc: 0.7420\n",
            "Epoch 82/150\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.7393 - acc: 0.7437\n",
            "Epoch 83/150\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.7336 - acc: 0.7459\n",
            "Epoch 84/150\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.7335 - acc: 0.7443\n",
            "Epoch 85/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7347 - acc: 0.7450\n",
            "Epoch 86/150\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.7335 - acc: 0.7454\n",
            "Epoch 87/150\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.7295 - acc: 0.7453\n",
            "Epoch 88/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7274 - acc: 0.7465\n",
            "Epoch 89/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.7272 - acc: 0.7471\n",
            "Epoch 90/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.7262 - acc: 0.7469\n",
            "Epoch 91/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7232 - acc: 0.7489\n",
            "Epoch 92/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7221 - acc: 0.7491\n",
            "Epoch 93/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7273 - acc: 0.7473\n",
            "Epoch 94/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7142 - acc: 0.7529\n",
            "Epoch 95/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7126 - acc: 0.7517\n",
            "Epoch 96/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7188 - acc: 0.7510\n",
            "Epoch 97/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7179 - acc: 0.7509\n",
            "Epoch 98/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7133 - acc: 0.7525\n",
            "Epoch 99/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7117 - acc: 0.7533\n",
            "Epoch 100/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.7092 - acc: 0.7537\n",
            "Epoch 101/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.7116 - acc: 0.7535\n",
            "Epoch 102/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6994 - acc: 0.7563\n",
            "Epoch 103/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7064 - acc: 0.7545\n",
            "Epoch 104/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.7108 - acc: 0.7552\n",
            "Epoch 105/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7056 - acc: 0.7533\n",
            "Epoch 106/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.7023 - acc: 0.7555\n",
            "Epoch 107/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.7017 - acc: 0.7566\n",
            "Epoch 108/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.7021 - acc: 0.7562\n",
            "Epoch 109/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7010 - acc: 0.7578\n",
            "Epoch 110/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.6984 - acc: 0.7580\n",
            "Epoch 111/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6946 - acc: 0.7580\n",
            "Epoch 112/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.6981 - acc: 0.7575\n",
            "Epoch 113/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.6976 - acc: 0.7584\n",
            "Epoch 114/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.6925 - acc: 0.7612\n",
            "Epoch 115/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.6891 - acc: 0.7591\n",
            "Epoch 116/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.6920 - acc: 0.7596\n",
            "Epoch 117/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6867 - acc: 0.7601\n",
            "Epoch 118/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6872 - acc: 0.7613\n",
            "Epoch 119/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6899 - acc: 0.7621\n",
            "Epoch 120/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6905 - acc: 0.7606\n",
            "Epoch 121/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6806 - acc: 0.7641\n",
            "Epoch 122/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6863 - acc: 0.7603\n",
            "Epoch 123/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6831 - acc: 0.7619\n",
            "Epoch 124/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6827 - acc: 0.7614\n",
            "Epoch 125/150\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.6753 - acc: 0.7655\n",
            "Epoch 126/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6813 - acc: 0.7648\n",
            "Epoch 127/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6831 - acc: 0.7644\n",
            "Epoch 128/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6776 - acc: 0.7634\n",
            "Epoch 129/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6778 - acc: 0.7637\n",
            "Epoch 130/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6788 - acc: 0.7647\n",
            "Epoch 131/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6728 - acc: 0.7679\n",
            "Epoch 132/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6766 - acc: 0.7628\n",
            "Epoch 133/150\n",
            "1250/1250 [==============================] - 39s 32ms/step - loss: 0.6719 - acc: 0.7670\n",
            "Epoch 134/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6726 - acc: 0.7664\n",
            "Epoch 135/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6733 - acc: 0.7666\n",
            "Epoch 136/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6756 - acc: 0.7655\n",
            "Epoch 137/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6666 - acc: 0.7691\n",
            "Epoch 138/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6696 - acc: 0.7681\n",
            "Epoch 139/150\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.6706 - acc: 0.7673\n",
            "Epoch 140/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6652 - acc: 0.7698\n",
            "Epoch 141/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6635 - acc: 0.7705\n",
            "Epoch 142/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6684 - acc: 0.7678\n",
            "Epoch 143/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6643 - acc: 0.7687\n",
            "Epoch 144/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6636 - acc: 0.7697\n",
            "Epoch 145/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6652 - acc: 0.7714\n",
            "Epoch 146/150\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 0.6654 - acc: 0.7678\n",
            "Epoch 147/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6677 - acc: 0.7675\n",
            "Epoch 148/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6584 - acc: 0.7711\n",
            "Epoch 149/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6650 - acc: 0.7711\n",
            "Epoch 150/150\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6609 - acc: 0.7703\n"
          ]
        }
      ],
      "source": [
        "# Train the model and store model parameters/loss values\n",
        "\n",
        "history_model_4 = model.fit(temp_data.flow(x_train, y_train_vec, batch_size=40), steps_per_epoch=x_tr.shape[0] // 32, epochs=150)\n",
        "model.save('model_4.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF5g8Ut5jVey"
      },
      "source": [
        "## Evaluate the model on the test set\n",
        "\n",
        "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCU9xHzrxfyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df08dd63-8a68-4646-856a-48337307f5d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 1.1030 - acc: 0.6632\n",
            "loss = 1.1030269861221313\n",
            "accuracy = 0.6632000207901001\n"
          ]
        }
      ],
      "source": [
        "# Evaluate your model performance (testing accuracy) on testing data.\n",
        "\n",
        "curr_model = load_model('model_3.h5')\n",
        "lacc = curr_model.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(lacc[0]))\n",
        "print('accuracy = ' + str(lacc[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQJdtdcUjmJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "039279fe-d934-41c6-e862-9825fa51f923"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 0.6504 - acc: 0.7792\n",
            "loss = 0.6504468321800232\n",
            "accuracy = 0.77920001745224\n"
          ]
        }
      ],
      "source": [
        "# Evaluate your model performance (testing accuracy) on testing data.\n",
        "\n",
        "curr_model = load_model('model_4.h5')\n",
        "lacc = curr_model.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(lacc[0]))\n",
        "print('accuracy = ' + str(lacc[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6maaDzvkJep"
      },
      "source": [
        "## Second Model using Batch Normalization and Dropout:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NK5g8oPUSUsh",
        "outputId": "2ac309d0-6b04-4481-a11c-a6f1351a0d3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 30, 30, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 30, 30, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 12, 12, 64)        32832     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 12, 12, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 12, 12, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2304)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2304)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               590080    \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 627,786\n",
            "Trainable params: 627,082\n",
            "Non-trainable params: 704\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Build the model\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (4, 4)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trBXxU8bUVtc"
      },
      "outputs": [],
      "source": [
        "# Define model optimizer and loss function\n",
        "\n",
        "lr = 0.0001\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(learning_rate=lr), metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yquECgX7VPs5",
        "outputId": "1ef6b059-f164-4892-c105-7e831c4e9df3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1000/1000 [==============================] - 24s 13ms/step - loss: 1.7040 - acc: 0.3880 - val_loss: 1.3660 - val_acc: 0.5193\n",
            "Epoch 2/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 1.3742 - acc: 0.5095 - val_loss: 1.1906 - val_acc: 0.5855\n",
            "Epoch 3/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 1.2522 - acc: 0.5553 - val_loss: 1.1191 - val_acc: 0.6135\n",
            "Epoch 4/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 1.1671 - acc: 0.5859 - val_loss: 1.0684 - val_acc: 0.6276\n",
            "Epoch 5/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 1.1046 - acc: 0.6089 - val_loss: 1.0469 - val_acc: 0.6293\n",
            "Epoch 6/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 1.0523 - acc: 0.6282 - val_loss: 1.0066 - val_acc: 0.6422\n",
            "Epoch 7/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 1.0140 - acc: 0.6443 - val_loss: 0.9483 - val_acc: 0.6655\n",
            "Epoch 8/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.9842 - acc: 0.6540 - val_loss: 0.9235 - val_acc: 0.6756\n",
            "Epoch 9/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.9523 - acc: 0.6636 - val_loss: 0.8803 - val_acc: 0.6930\n",
            "Epoch 10/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.9250 - acc: 0.6772 - val_loss: 0.9160 - val_acc: 0.6748\n",
            "Epoch 11/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.9027 - acc: 0.6822 - val_loss: 0.9591 - val_acc: 0.6602\n",
            "Epoch 12/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.8807 - acc: 0.6909 - val_loss: 0.8610 - val_acc: 0.6982\n",
            "Epoch 13/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.8621 - acc: 0.6963 - val_loss: 0.8488 - val_acc: 0.6997\n",
            "Epoch 14/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.8435 - acc: 0.7062 - val_loss: 0.8221 - val_acc: 0.7056\n",
            "Epoch 15/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.8195 - acc: 0.7138 - val_loss: 0.8080 - val_acc: 0.7180\n",
            "Epoch 16/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.8013 - acc: 0.7219 - val_loss: 0.8280 - val_acc: 0.7148\n",
            "Epoch 17/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.7827 - acc: 0.7292 - val_loss: 0.8167 - val_acc: 0.7087\n",
            "Epoch 18/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.7777 - acc: 0.7288 - val_loss: 0.7724 - val_acc: 0.7322\n",
            "Epoch 19/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.7606 - acc: 0.7362 - val_loss: 0.7725 - val_acc: 0.7342\n",
            "Epoch 20/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.7417 - acc: 0.7424 - val_loss: 0.7946 - val_acc: 0.7286\n",
            "Epoch 21/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.7319 - acc: 0.7450 - val_loss: 0.7899 - val_acc: 0.7227\n",
            "Epoch 22/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.7154 - acc: 0.7516 - val_loss: 0.7483 - val_acc: 0.7417\n",
            "Epoch 23/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.7046 - acc: 0.7533 - val_loss: 0.7359 - val_acc: 0.7460\n",
            "Epoch 24/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.6928 - acc: 0.7580 - val_loss: 0.8648 - val_acc: 0.7117\n",
            "Epoch 25/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.6738 - acc: 0.7659 - val_loss: 0.7693 - val_acc: 0.7328\n",
            "Epoch 26/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.6667 - acc: 0.7703 - val_loss: 0.7154 - val_acc: 0.7566\n",
            "Epoch 27/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.6599 - acc: 0.7716 - val_loss: 0.7636 - val_acc: 0.7341\n",
            "Epoch 28/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.6474 - acc: 0.7761 - val_loss: 0.7245 - val_acc: 0.7493\n",
            "Epoch 29/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.6364 - acc: 0.7790 - val_loss: 0.7019 - val_acc: 0.7572\n",
            "Epoch 30/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.6241 - acc: 0.7822 - val_loss: 0.7398 - val_acc: 0.7423\n",
            "Epoch 31/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.6146 - acc: 0.7866 - val_loss: 0.7429 - val_acc: 0.7441\n",
            "Epoch 32/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.6092 - acc: 0.7884 - val_loss: 0.7060 - val_acc: 0.7582\n",
            "Epoch 33/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.5970 - acc: 0.7933 - val_loss: 0.7214 - val_acc: 0.7545\n",
            "Epoch 34/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.5849 - acc: 0.7966 - val_loss: 0.7178 - val_acc: 0.7543\n",
            "Epoch 35/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5795 - acc: 0.7994 - val_loss: 0.6954 - val_acc: 0.7585\n",
            "Epoch 36/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5717 - acc: 0.8003 - val_loss: 0.7419 - val_acc: 0.7456\n",
            "Epoch 37/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5615 - acc: 0.8029 - val_loss: 0.6835 - val_acc: 0.7693\n",
            "Epoch 38/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5518 - acc: 0.8084 - val_loss: 0.6824 - val_acc: 0.7659\n",
            "Epoch 39/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5446 - acc: 0.8111 - val_loss: 0.6702 - val_acc: 0.7682\n",
            "Epoch 40/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5382 - acc: 0.8149 - val_loss: 0.6832 - val_acc: 0.7683\n",
            "Epoch 41/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5278 - acc: 0.8174 - val_loss: 0.7126 - val_acc: 0.7589\n",
            "Epoch 42/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5197 - acc: 0.8186 - val_loss: 0.7524 - val_acc: 0.7481\n",
            "Epoch 43/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5114 - acc: 0.8225 - val_loss: 0.7067 - val_acc: 0.7574\n",
            "Epoch 44/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5062 - acc: 0.8236 - val_loss: 0.7460 - val_acc: 0.7465\n",
            "Epoch 45/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.5020 - acc: 0.8258 - val_loss: 0.6656 - val_acc: 0.7752\n",
            "Epoch 46/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4925 - acc: 0.8275 - val_loss: 0.6965 - val_acc: 0.7640\n",
            "Epoch 47/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4869 - acc: 0.8332 - val_loss: 0.7278 - val_acc: 0.7536\n",
            "Epoch 48/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4773 - acc: 0.8352 - val_loss: 0.6779 - val_acc: 0.7696\n",
            "Epoch 49/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4746 - acc: 0.8363 - val_loss: 0.7929 - val_acc: 0.7369\n",
            "Epoch 50/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4649 - acc: 0.8406 - val_loss: 0.7003 - val_acc: 0.7598\n",
            "Epoch 51/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4617 - acc: 0.8422 - val_loss: 0.7338 - val_acc: 0.7508\n",
            "Epoch 52/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4546 - acc: 0.8450 - val_loss: 0.7171 - val_acc: 0.7514\n",
            "Epoch 53/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4432 - acc: 0.8462 - val_loss: 0.6712 - val_acc: 0.7703\n",
            "Epoch 54/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4422 - acc: 0.8472 - val_loss: 0.6414 - val_acc: 0.7814\n",
            "Epoch 55/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4398 - acc: 0.8485 - val_loss: 0.6455 - val_acc: 0.7779\n",
            "Epoch 56/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4284 - acc: 0.8510 - val_loss: 0.6621 - val_acc: 0.7787\n",
            "Epoch 57/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4242 - acc: 0.8547 - val_loss: 0.8166 - val_acc: 0.7352\n",
            "Epoch 58/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4181 - acc: 0.8570 - val_loss: 0.6570 - val_acc: 0.7796\n",
            "Epoch 59/100\n",
            "1000/1000 [==============================] - 16s 16ms/step - loss: 0.4140 - acc: 0.8557 - val_loss: 0.6628 - val_acc: 0.7750\n",
            "Epoch 60/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.4062 - acc: 0.8604 - val_loss: 0.7828 - val_acc: 0.7464\n",
            "Epoch 61/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.4058 - acc: 0.8601 - val_loss: 0.7846 - val_acc: 0.7423\n",
            "Epoch 62/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3975 - acc: 0.8644 - val_loss: 0.7174 - val_acc: 0.7660\n",
            "Epoch 63/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3905 - acc: 0.8640 - val_loss: 0.6950 - val_acc: 0.7666\n",
            "Epoch 64/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3935 - acc: 0.8661 - val_loss: 0.7038 - val_acc: 0.7708\n",
            "Epoch 65/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3878 - acc: 0.8654 - val_loss: 0.6466 - val_acc: 0.7815\n",
            "Epoch 66/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3813 - acc: 0.8682 - val_loss: 0.6414 - val_acc: 0.7849\n",
            "Epoch 67/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3835 - acc: 0.8683 - val_loss: 0.7575 - val_acc: 0.7544\n",
            "Epoch 68/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3722 - acc: 0.8713 - val_loss: 0.6492 - val_acc: 0.7849\n",
            "Epoch 69/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3689 - acc: 0.8728 - val_loss: 0.7663 - val_acc: 0.7502\n",
            "Epoch 70/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3640 - acc: 0.8756 - val_loss: 0.6564 - val_acc: 0.7892\n",
            "Epoch 71/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3609 - acc: 0.8763 - val_loss: 0.6462 - val_acc: 0.7866\n",
            "Epoch 72/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3580 - acc: 0.8770 - val_loss: 0.6548 - val_acc: 0.7830\n",
            "Epoch 73/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3515 - acc: 0.8797 - val_loss: 0.7387 - val_acc: 0.7681\n",
            "Epoch 74/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3486 - acc: 0.8799 - val_loss: 0.6677 - val_acc: 0.7770\n",
            "Epoch 75/100\n",
            "1000/1000 [==============================] - 14s 14ms/step - loss: 0.3437 - acc: 0.8801 - val_loss: 0.6879 - val_acc: 0.7795\n",
            "Epoch 76/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3342 - acc: 0.8855 - val_loss: 0.7173 - val_acc: 0.7605\n",
            "Epoch 77/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3361 - acc: 0.8865 - val_loss: 0.6395 - val_acc: 0.7866\n",
            "Epoch 78/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3317 - acc: 0.8866 - val_loss: 0.6860 - val_acc: 0.7805\n",
            "Epoch 79/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3289 - acc: 0.8874 - val_loss: 0.7070 - val_acc: 0.7640\n",
            "Epoch 80/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3267 - acc: 0.8874 - val_loss: 0.6917 - val_acc: 0.7788\n",
            "Epoch 81/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3230 - acc: 0.8886 - val_loss: 0.7223 - val_acc: 0.7748\n",
            "Epoch 82/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3233 - acc: 0.8882 - val_loss: 0.6497 - val_acc: 0.7900\n",
            "Epoch 83/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3174 - acc: 0.8918 - val_loss: 0.7188 - val_acc: 0.7695\n",
            "Epoch 84/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3116 - acc: 0.8921 - val_loss: 0.6681 - val_acc: 0.7771\n",
            "Epoch 85/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3132 - acc: 0.8927 - val_loss: 0.6722 - val_acc: 0.7865\n",
            "Epoch 86/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3082 - acc: 0.8955 - val_loss: 0.7259 - val_acc: 0.7734\n",
            "Epoch 87/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3065 - acc: 0.8947 - val_loss: 0.7262 - val_acc: 0.7753\n",
            "Epoch 88/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3085 - acc: 0.8938 - val_loss: 0.6651 - val_acc: 0.7862\n",
            "Epoch 89/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.3009 - acc: 0.8980 - val_loss: 0.6699 - val_acc: 0.7821\n",
            "Epoch 90/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2962 - acc: 0.8994 - val_loss: 0.6654 - val_acc: 0.7804\n",
            "Epoch 91/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2915 - acc: 0.8985 - val_loss: 0.6982 - val_acc: 0.7791\n",
            "Epoch 92/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2904 - acc: 0.9004 - val_loss: 0.6765 - val_acc: 0.7806\n",
            "Epoch 93/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2921 - acc: 0.8965 - val_loss: 0.6644 - val_acc: 0.7817\n",
            "Epoch 94/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2833 - acc: 0.9026 - val_loss: 0.6792 - val_acc: 0.7843\n",
            "Epoch 95/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2838 - acc: 0.9017 - val_loss: 0.6548 - val_acc: 0.7873\n",
            "Epoch 96/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2793 - acc: 0.9047 - val_loss: 0.7371 - val_acc: 0.7608\n",
            "Epoch 97/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2793 - acc: 0.9039 - val_loss: 0.6673 - val_acc: 0.7877\n",
            "Epoch 98/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2770 - acc: 0.9036 - val_loss: 0.8245 - val_acc: 0.7562\n",
            "Epoch 99/100\n",
            "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2728 - acc: 0.9060 - val_loss: 0.7120 - val_acc: 0.7789\n",
            "Epoch 100/100\n",
            "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2736 - acc: 0.9067 - val_loss: 0.6660 - val_acc: 0.7850\n"
          ]
        }
      ],
      "source": [
        "# Train the model and store model parameters/loss values\n",
        "\n",
        "history_model_5 = model.fit(x_tr, y_tr, batch_size=40, epochs=100, validation_data=(x_val, y_val))\n",
        "model.save('model_5.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvB3zvKHlq0b"
      },
      "source": [
        "## Plot the training and validation loss curve versus epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGAjrUIOlvvH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "a0537781-633c-4f6a-9c53-bac6bc806762"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Plot the loss curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history_model_5.history['acc']\n",
        "val_acc = history_model_5.history['val_acc']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaL358TXmgcA"
      },
      "source": [
        "## Train (again) and evaluate the model\n",
        "- To this end, you have found the \"best\" hyper-parameters. \n",
        "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
        "- Evaluate your model on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMWTGOEWmk6_"
      },
      "source": [
        "### Train the model on the entire training set\n",
        "\n",
        "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfPOOPNpn_wO"
      },
      "outputs": [],
      "source": [
        "#<Compile your model again (using the same hyper-parameters you tuned above)>\n",
        "\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (4, 4)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(learning_rate=lr), metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model and store model parameters/loss values\n",
        "\n",
        "history_model_6 = model.fit(x_train, y_train_vec, batch_size=40, epochs=100)\n",
        "model.save('model_6.h5')"
      ],
      "metadata": {
        "id": "m9mxYJEmxEyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa6393c1-12c0-4e17-ec9d-5c2555204631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1250/1250 [==============================] - 25s 11ms/step - loss: 1.6303 - acc: 0.4181\n",
            "Epoch 2/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.2876 - acc: 0.5449\n",
            "Epoch 3/100\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 1.1565 - acc: 0.5913\n",
            "Epoch 4/100\n",
            "1250/1250 [==============================] - 19s 15ms/step - loss: 1.0758 - acc: 0.6216\n",
            "Epoch 5/100\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 1.0230 - acc: 0.6397\n",
            "Epoch 6/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9755 - acc: 0.6581\n",
            "Epoch 7/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9426 - acc: 0.6685\n",
            "Epoch 8/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9091 - acc: 0.6832\n",
            "Epoch 9/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8833 - acc: 0.6879\n",
            "Epoch 10/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8603 - acc: 0.6982\n",
            "Epoch 11/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8399 - acc: 0.7050\n",
            "Epoch 12/100\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.8178 - acc: 0.7133\n",
            "Epoch 13/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8020 - acc: 0.7199\n",
            "Epoch 14/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7809 - acc: 0.7271\n",
            "Epoch 15/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7633 - acc: 0.7349\n",
            "Epoch 16/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7480 - acc: 0.7370\n",
            "Epoch 17/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7313 - acc: 0.7453\n",
            "Epoch 18/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7196 - acc: 0.7480\n",
            "Epoch 19/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7084 - acc: 0.7518\n",
            "Epoch 20/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6898 - acc: 0.7598\n",
            "Epoch 21/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6831 - acc: 0.7640\n",
            "Epoch 22/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6692 - acc: 0.7674\n",
            "Epoch 23/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6527 - acc: 0.7732\n",
            "Epoch 24/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6452 - acc: 0.7745\n",
            "Epoch 25/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6336 - acc: 0.7797\n",
            "Epoch 26/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6254 - acc: 0.7822\n",
            "Epoch 27/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6170 - acc: 0.7857\n",
            "Epoch 28/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6048 - acc: 0.7900\n",
            "Epoch 29/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5988 - acc: 0.7943\n",
            "Epoch 30/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5889 - acc: 0.7967\n",
            "Epoch 31/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5793 - acc: 0.7997\n",
            "Epoch 32/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5691 - acc: 0.8041\n",
            "Epoch 33/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5627 - acc: 0.8059\n",
            "Epoch 34/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5530 - acc: 0.8096\n",
            "Epoch 35/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5415 - acc: 0.8137\n",
            "Epoch 36/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5391 - acc: 0.8131\n",
            "Epoch 37/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5339 - acc: 0.8150\n",
            "Epoch 38/100\n",
            "1250/1250 [==============================] - 14s 12ms/step - loss: 0.5191 - acc: 0.8209\n",
            "Epoch 39/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5129 - acc: 0.8225\n",
            "Epoch 40/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5065 - acc: 0.8242\n",
            "Epoch 41/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.5001 - acc: 0.8296\n",
            "Epoch 42/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4940 - acc: 0.8297\n",
            "Epoch 43/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4888 - acc: 0.8325\n",
            "Epoch 44/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4829 - acc: 0.8311\n",
            "Epoch 45/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4777 - acc: 0.8358\n",
            "Epoch 46/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4648 - acc: 0.8390\n",
            "Epoch 47/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4672 - acc: 0.8364\n",
            "Epoch 48/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4605 - acc: 0.8427\n",
            "Epoch 49/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4491 - acc: 0.8439\n",
            "Epoch 50/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4485 - acc: 0.8450\n",
            "Epoch 51/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4402 - acc: 0.8476\n",
            "Epoch 52/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4367 - acc: 0.8486\n",
            "Epoch 53/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4310 - acc: 0.8516\n",
            "Epoch 54/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4253 - acc: 0.8529\n",
            "Epoch 55/100\n",
            "1250/1250 [==============================] - 14s 12ms/step - loss: 0.4239 - acc: 0.8534\n",
            "Epoch 56/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4112 - acc: 0.8586\n",
            "Epoch 57/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4120 - acc: 0.8574\n",
            "Epoch 58/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4062 - acc: 0.8596\n",
            "Epoch 59/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4005 - acc: 0.8622\n",
            "Epoch 60/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3974 - acc: 0.8636\n",
            "Epoch 61/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3915 - acc: 0.8639\n",
            "Epoch 62/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3917 - acc: 0.8652\n",
            "Epoch 63/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3876 - acc: 0.8667\n",
            "Epoch 64/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3797 - acc: 0.8708\n",
            "Epoch 65/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3770 - acc: 0.8693\n",
            "Epoch 66/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3730 - acc: 0.8720\n",
            "Epoch 67/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3690 - acc: 0.8735\n",
            "Epoch 68/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3669 - acc: 0.8745\n",
            "Epoch 69/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3587 - acc: 0.8750\n",
            "Epoch 70/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3580 - acc: 0.8758\n",
            "Epoch 71/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3529 - acc: 0.8785\n",
            "Epoch 72/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3522 - acc: 0.8796\n",
            "Epoch 73/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3490 - acc: 0.8777\n",
            "Epoch 74/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3459 - acc: 0.8803\n",
            "Epoch 75/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3412 - acc: 0.8820\n",
            "Epoch 76/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3369 - acc: 0.8832\n",
            "Epoch 77/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3322 - acc: 0.8862\n",
            "Epoch 78/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3378 - acc: 0.8815\n",
            "Epoch 79/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3275 - acc: 0.8862\n",
            "Epoch 80/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3235 - acc: 0.8893\n",
            "Epoch 81/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3226 - acc: 0.8878\n",
            "Epoch 82/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3160 - acc: 0.8907\n",
            "Epoch 83/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3157 - acc: 0.8912\n",
            "Epoch 84/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3106 - acc: 0.8941\n",
            "Epoch 85/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3135 - acc: 0.8921\n",
            "Epoch 86/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3129 - acc: 0.8921\n",
            "Epoch 87/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3101 - acc: 0.8931\n",
            "Epoch 88/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3087 - acc: 0.8917\n",
            "Epoch 89/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3013 - acc: 0.8952\n",
            "Epoch 90/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3025 - acc: 0.8968\n",
            "Epoch 91/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3011 - acc: 0.8956\n",
            "Epoch 92/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3005 - acc: 0.8959\n",
            "Epoch 93/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.2983 - acc: 0.8971\n",
            "Epoch 94/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.2918 - acc: 0.8994\n",
            "Epoch 95/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.2933 - acc: 0.8979\n",
            "Epoch 96/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.2910 - acc: 0.8997\n",
            "Epoch 97/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.2882 - acc: 0.9018\n",
            "Epoch 98/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.2862 - acc: 0.9011\n",
            "Epoch 99/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.2822 - acc: 0.9021\n",
            "Epoch 100/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.2813 - acc: 0.9018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2u-gWd8kQsn"
      },
      "source": [
        "##  Evaluate the model on the test set\n",
        "\n",
        "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8k7gqRrkZIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98ef1dd9-79c4-431c-d8de-042922a7de56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 0.6877 - acc: 0.7817\n",
            "loss = 0.6877126097679138\n",
            "accuracy = 0.7817000150680542\n"
          ]
        }
      ],
      "source": [
        "# Evaluate your model performance (testing accuracy) on testing data.\n",
        "\n",
        "curr_model = load_model('model_5.h5')\n",
        "lacc = curr_model.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(lacc[0]))\n",
        "print('accuracy = ' + str(lacc[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEDGxtHTkdgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dc7bdd3-19ea-44a8-b9b1-d8b9d3df541f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 0.6445 - acc: 0.7943\n",
            "loss = 0.6444640159606934\n",
            "accuracy = 0.7943000197410583\n"
          ]
        }
      ],
      "source": [
        "# Evaluate your model performance (testing accuracy) on testing data.\n",
        "\n",
        "curr_model = load_model('model_6.h5')\n",
        "lacc = curr_model.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(lacc[0]))\n",
        "print('accuracy = ' + str(lacc[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparision and Analysis:"
      ],
      "metadata": {
        "id": "mtbj2BDevXsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Normal CNN model:*** is not that efficient and has an accuracy of 69 % and after using full training dataset it achieves accuracy of 70 %.\n",
        "***My first model*** which uses data augmentation and batch normalization has a accuracy of 66 % and after using full training dataset it achieves has accuracy of ~78 %. \n",
        "***My second and final model*** which uses batch normaliztion and dropout has an accuracy of 78 % and after using full training dataset it achieves an accuracy of ~80 %.\n",
        "\n",
        "***Hence, I think model 2 is the best model, I could make considering the time and the limited availability of Google Colab's GPU feature.***\n"
      ],
      "metadata": {
        "id": "uCsG0mvrveLz"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}