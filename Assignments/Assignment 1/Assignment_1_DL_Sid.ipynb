{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bxRfF9ZH5Nr"
      },
      "source": [
        "# HM1: Logistic Regression.\n",
        "\n",
        "### Name: Siddharth Harsukh Pansuria\n",
        "### CWID: 20005837"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDr5TGIgH5Nx"
      },
      "source": [
        "#### For this assignment, you will build 6 models. You need to train Logistic Regression/Regularized Logistic Regression each with Batch Gradient Descent, Stochastic Gradient Descent and Mini Batch Gradient Descent. Also you should plot their objective values versus epochs and compare their training and testing accuracies. You will need to tune the parameters a little bit to obtain reasonable results.\n",
        "\n",
        "#### You do not have to follow the following procedure. You may implement your own functions and methods, but you need to show your results and plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {
        "id": "9cM634U_H5Ny"
      },
      "outputs": [],
      "source": [
        "# Load Packages\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl-CK-z1H5N0"
      },
      "source": [
        "# 1. Data processing\n",
        "\n",
        "- Download the Breast Cancer dataset from canvas or from https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)\n",
        "- Load the data.\n",
        "- Preprocess the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4WZobXiH5N1"
      },
      "source": [
        "## 1.1. Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "MzyY0i9-H5N1",
        "outputId": "05dc6776-3ac8-472f-cbb3-a4273c9f7c75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-423a6086-1117-4bf0-ae79-c7f0193fe2b4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>926424</td>\n",
              "      <td>M</td>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>1.1760</td>\n",
              "      <td>1.2560</td>\n",
              "      <td>7.673</td>\n",
              "      <td>158.70</td>\n",
              "      <td>0.010300</td>\n",
              "      <td>0.02891</td>\n",
              "      <td>0.05198</td>\n",
              "      <td>0.02454</td>\n",
              "      <td>0.01114</td>\n",
              "      <td>0.004239</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>926682</td>\n",
              "      <td>M</td>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>0.7655</td>\n",
              "      <td>2.4630</td>\n",
              "      <td>5.203</td>\n",
              "      <td>99.04</td>\n",
              "      <td>0.005769</td>\n",
              "      <td>0.02423</td>\n",
              "      <td>0.03950</td>\n",
              "      <td>0.01678</td>\n",
              "      <td>0.01898</td>\n",
              "      <td>0.002498</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>926954</td>\n",
              "      <td>M</td>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>0.4564</td>\n",
              "      <td>1.0750</td>\n",
              "      <td>3.425</td>\n",
              "      <td>48.55</td>\n",
              "      <td>0.005903</td>\n",
              "      <td>0.03731</td>\n",
              "      <td>0.04730</td>\n",
              "      <td>0.01557</td>\n",
              "      <td>0.01318</td>\n",
              "      <td>0.003892</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>927241</td>\n",
              "      <td>M</td>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>0.7260</td>\n",
              "      <td>1.5950</td>\n",
              "      <td>5.772</td>\n",
              "      <td>86.22</td>\n",
              "      <td>0.006522</td>\n",
              "      <td>0.06158</td>\n",
              "      <td>0.07117</td>\n",
              "      <td>0.01664</td>\n",
              "      <td>0.02324</td>\n",
              "      <td>0.006185</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>92751</td>\n",
              "      <td>B</td>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>1.4280</td>\n",
              "      <td>2.548</td>\n",
              "      <td>19.15</td>\n",
              "      <td>0.007189</td>\n",
              "      <td>0.00466</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.02676</td>\n",
              "      <td>0.002783</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 33 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-423a6086-1117-4bf0-ae79-c7f0193fe2b4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-423a6086-1117-4bf0-ae79-c7f0193fe2b4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-423a6086-1117-4bf0-ae79-c7f0193fe2b4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0      842302         M  ...                  0.11890          NaN\n",
              "1      842517         M  ...                  0.08902          NaN\n",
              "2    84300903         M  ...                  0.08758          NaN\n",
              "3    84348301         M  ...                  0.17300          NaN\n",
              "4    84358402         M  ...                  0.07678          NaN\n",
              "..        ...       ...  ...                      ...          ...\n",
              "564    926424         M  ...                  0.07115          NaN\n",
              "565    926682         M  ...                  0.06637          NaN\n",
              "566    926954         M  ...                  0.07820          NaN\n",
              "567    927241         M  ...                  0.12400          NaN\n",
              "568     92751         B  ...                  0.07039          NaN\n",
              "\n",
              "[569 rows x 33 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 269
        }
      ],
      "source": [
        "# Load the data as dataframe\n",
        "df = pd.read_csv('data.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ua0Uty7H5N2"
      },
      "source": [
        "## 1.2 Examine and clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "wUoQ-3aBH5N3",
        "outputId": "293f8cd0-5f20-4417-b428-a17de73d1b3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-103b4e98-d8b0-4137-b60f-75630a4c39e8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>-1</td>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>1.1760</td>\n",
              "      <td>1.2560</td>\n",
              "      <td>7.673</td>\n",
              "      <td>158.70</td>\n",
              "      <td>0.010300</td>\n",
              "      <td>0.02891</td>\n",
              "      <td>0.05198</td>\n",
              "      <td>0.02454</td>\n",
              "      <td>0.01114</td>\n",
              "      <td>0.004239</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>-1</td>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>0.7655</td>\n",
              "      <td>2.4630</td>\n",
              "      <td>5.203</td>\n",
              "      <td>99.04</td>\n",
              "      <td>0.005769</td>\n",
              "      <td>0.02423</td>\n",
              "      <td>0.03950</td>\n",
              "      <td>0.01678</td>\n",
              "      <td>0.01898</td>\n",
              "      <td>0.002498</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>-1</td>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>0.4564</td>\n",
              "      <td>1.0750</td>\n",
              "      <td>3.425</td>\n",
              "      <td>48.55</td>\n",
              "      <td>0.005903</td>\n",
              "      <td>0.03731</td>\n",
              "      <td>0.04730</td>\n",
              "      <td>0.01557</td>\n",
              "      <td>0.01318</td>\n",
              "      <td>0.003892</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>-1</td>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>0.7260</td>\n",
              "      <td>1.5950</td>\n",
              "      <td>5.772</td>\n",
              "      <td>86.22</td>\n",
              "      <td>0.006522</td>\n",
              "      <td>0.06158</td>\n",
              "      <td>0.07117</td>\n",
              "      <td>0.01664</td>\n",
              "      <td>0.02324</td>\n",
              "      <td>0.006185</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>1</td>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>1.4280</td>\n",
              "      <td>2.548</td>\n",
              "      <td>19.15</td>\n",
              "      <td>0.007189</td>\n",
              "      <td>0.00466</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.02676</td>\n",
              "      <td>0.002783</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-103b4e98-d8b0-4137-b60f-75630a4c39e8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-103b4e98-d8b0-4137-b60f-75630a4c39e8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-103b4e98-d8b0-4137-b60f-75630a4c39e8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     diagnosis  radius_mean  ...  symmetry_worst  fractal_dimension_worst\n",
              "0           -1        17.99  ...          0.4601                  0.11890\n",
              "1           -1        20.57  ...          0.2750                  0.08902\n",
              "2           -1        19.69  ...          0.3613                  0.08758\n",
              "3           -1        11.42  ...          0.6638                  0.17300\n",
              "4           -1        20.29  ...          0.2364                  0.07678\n",
              "..         ...          ...  ...             ...                      ...\n",
              "564         -1        21.56  ...          0.2060                  0.07115\n",
              "565         -1        20.13  ...          0.2572                  0.06637\n",
              "566         -1        16.60  ...          0.2218                  0.07820\n",
              "567         -1        20.60  ...          0.4087                  0.12400\n",
              "568          1         7.76  ...          0.2871                  0.07039\n",
              "\n",
              "[569 rows x 31 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 270
        }
      ],
      "source": [
        "# Some columns may not be useful for the model (For example, the first column contains ID number which may be irrelavant). \n",
        "# You need to get rid of the ID number feature.\n",
        "# Also you should transform target labels in the second column from 'B' and 'M' to 1 and -1.\n",
        "df.drop(df.columns[[0, 32]], axis = 1, inplace = True) # Dropping unnecessary columns\n",
        "df[\"diagnosis\"].replace({\"M\": -1, \"B\": 1}, inplace = True) # Replacing M with -1 and B with 1\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P1qezGxH5N4"
      },
      "source": [
        "## 1.3. Partition to training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "id": "vp5U2dJfH5N5"
      },
      "outputs": [],
      "source": [
        "# You can partition using 80% training data and 20% testing data. It is a commonly used ratio in machine learning.\n",
        "X, y = np.asarray(df.iloc[:,1:]), np.asarray(df.iloc[:,0:1]) # Seperating features and target columns\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20) # Test train split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZhHdeAUH5N6"
      },
      "source": [
        "## 1.4. Feature scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrHrh2c_H5N6"
      },
      "source": [
        "Use the standardization to trainsform both training and test features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ACnYY_zH5N6",
        "outputId": "dc5216f8-b32d-43ec-b614-ef5f5a7e8583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test mean = \n",
            "[ 0.0128982   0.09619309  0.02153908  0.0062616  -0.00284483  0.1218092\n",
            "  0.11497821  0.04729724  0.08677152  0.0381384  -0.03422155 -0.02598006\n",
            " -0.01661957 -0.00142957 -0.09457139  0.04550315  0.16609029  0.06389974\n",
            " -0.11113464  0.06461517  0.0214923   0.14296148  0.03052313  0.01442689\n",
            "  0.10185983  0.19346888  0.22484819  0.14075701  0.17558728  0.16539207]\n",
            "test std = \n",
            "[0.96739005 1.00854412 0.971261   1.02119096 0.86177632 1.07236923\n",
            " 1.04034312 1.01726579 0.99969559 1.02281423 1.10076206 0.96060541\n",
            " 1.09820197 1.30299171 0.74997806 0.92747851 1.48905298 1.12928124\n",
            " 0.77970096 1.33413419 0.98381813 0.97875956 0.9854047  1.04693985\n",
            " 0.97307143 1.10571111 1.06387424 1.03224208 1.0831142  1.08729588]\n"
          ]
        }
      ],
      "source": [
        "# Standardization\n",
        "import numpy\n",
        "\n",
        "# calculate mu and sig using the training set\n",
        "d = x_train.shape[1]\n",
        "mu = numpy.mean(x_train, axis=0).reshape(1, d)\n",
        "sig = numpy.std(x_train, axis=0).reshape(1, d)\n",
        "\n",
        "# transform the training features\n",
        "x_train = (x_train - mu) / (sig + 1E-6)\n",
        "\n",
        "# transform the test features\n",
        "x_test = (x_test - mu) / (sig + 1E-6)\n",
        "\n",
        "print('test mean = ')\n",
        "print(numpy.mean(x_test, axis=0))\n",
        "\n",
        "print('test std = ')\n",
        "print(numpy.std(x_test, axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSufF9V1H5N7"
      },
      "source": [
        "# 2.  Logistic Regression Model\n",
        "\n",
        "The objective function is $Q (w; X, y) = \\frac{1}{n} \\sum_{i=1}^n \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
        "\n",
        "When $\\lambda = 0$, the model is a regular logistric regression and when $\\lambda > 0$, it essentially becomes a regularized logistric regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "id": "OMwLwPrHH5N7"
      },
      "outputs": [],
      "source": [
        "# Calculate the objective function value, or loss\n",
        "# Inputs:\n",
        "#     w: weight: d-by-1 matrix\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: regularization parameter: scalar\n",
        "# Return:\n",
        "#     objective function value, or loss (scalar)\n",
        "def objective(w, x, y, lam):\n",
        "\n",
        "  yxw = np.dot(np.multiply(y,x),w)\n",
        "  expTerm = np.exp(-yxw)\n",
        "  \n",
        "  firstObj = np.mean(np.log(1 + expTerm))\n",
        "  secondObj = (lam / 2) * np.sum(w * w)\n",
        "  \n",
        "  objval = firstObj + secondObj\n",
        "  \n",
        "  return objval\n",
        "\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_W3GVhHH5N7"
      },
      "source": [
        "# 3. Numerical optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxViSINSH5N8"
      },
      "source": [
        "## 3.1. Gradient descent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNWIu9OAH5N8"
      },
      "source": [
        "The gradient at $w$ for regularized logistic regression is  $g = - \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "id": "gZy7A0XiH5N8"
      },
      "outputs": [],
      "source": [
        "# Calculate the gradient\n",
        "# Inputs:\n",
        "#     w: weight: d-by-1 matrix\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: regularization parameter: scalar\n",
        "# Return:\n",
        "#     g: gradient: d-by-1 matrix\n",
        "\n",
        "def gradient(w, x, y, lam):\n",
        "\n",
        "  yx = np.multiply(y, x)\n",
        "  expTerm = np.exp(np.dot(yx, w))\n",
        "  \n",
        "  firstG = -np.mean(np.divide(yx, 1 + expTerm),axis = 0).reshape(d,1)\n",
        "  secondG = lam * w\n",
        "  \n",
        "  g = firstG + secondG\n",
        "  \n",
        "  return g\n",
        "  \n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "id": "BJGHCfUOH5N9"
      },
      "outputs": [],
      "source": [
        "# Gradient descent for solving logistic regression\n",
        "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
        "\n",
        "# Inputs:\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: scalar, the regularization parameter\n",
        "#     learning_rate: scalar\n",
        "#     w: weights: d-by-1 matrix, initialization of w\n",
        "#     max_epoch: integer, the maximal epochs\n",
        "# Return:\n",
        "#     w: weights: d-by-1 matrix, the solution\n",
        "#     objvals: a record of each epoch's objective value\n",
        "\n",
        "def gradient_descent(x, y, lam, learning_rate, w, max_epoch=100):\n",
        "  \n",
        "  objvals = np.zeros(max_epoch) # making an array to store objective values\n",
        "\n",
        "  for i in range(0,max_epoch) :\n",
        "\n",
        "    currentObjectiveValue = objective(w, x, y, lam) \n",
        "    currentGradient = gradient(w, x, y, lam)\n",
        "\n",
        "    w -= learning_rate  * currentGradient # updating weight\n",
        "\n",
        "    objvals[i] = currentObjectiveValue \n",
        "\n",
        "    print(f'Iteration number: {i+1} Current Objective Value: {objvals[i]}')\n",
        "\n",
        "  return w, objvals\n",
        "  \n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3QS9SojH5N9"
      },
      "source": [
        "Use gradient_descent function to obtain your optimal weights and a list of objective values over each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju8MvMZuH5N9",
        "outputId": "9519a619-7f00-417f-ab3f-30e8372a16de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration number: 1 Current Objective Value: 0.6931471805599453\n",
            "Iteration number: 2 Current Objective Value: 0.15921984932770258\n",
            "Iteration number: 3 Current Objective Value: 0.13025952789414766\n",
            "Iteration number: 4 Current Objective Value: 0.11528716422134247\n",
            "Iteration number: 5 Current Objective Value: 0.10668482073983347\n",
            "Iteration number: 6 Current Objective Value: 0.10099807261569023\n",
            "Iteration number: 7 Current Objective Value: 0.0967092352354366\n",
            "Iteration number: 8 Current Objective Value: 0.09320439824261602\n",
            "Iteration number: 9 Current Objective Value: 0.09021615517602312\n",
            "Iteration number: 10 Current Objective Value: 0.08760692413077088\n",
            "Iteration number: 11 Current Objective Value: 0.08529379620816697\n",
            "Iteration number: 12 Current Objective Value: 0.08322076253267549\n",
            "Iteration number: 13 Current Objective Value: 0.08134708671403236\n",
            "Iteration number: 14 Current Objective Value: 0.07964172010100179\n",
            "Iteration number: 15 Current Objective Value: 0.0780802575256351\n",
            "Iteration number: 16 Current Objective Value: 0.07664309702149007\n",
            "Iteration number: 17 Current Objective Value: 0.07531423738594492\n",
            "Iteration number: 18 Current Objective Value: 0.07408044728580741\n",
            "Iteration number: 19 Current Objective Value: 0.07293066767522345\n",
            "Iteration number: 20 Current Objective Value: 0.07185556929627226\n",
            "Iteration number: 21 Current Objective Value: 0.07084721771762846\n",
            "Iteration number: 22 Current Objective Value: 0.06989881534305295\n",
            "Iteration number: 23 Current Objective Value: 0.06900449986417798\n",
            "Iteration number: 24 Current Objective Value: 0.06815918490716733\n",
            "Iteration number: 25 Current Objective Value: 0.06735843271851275\n",
            "Iteration number: 26 Current Objective Value: 0.06659835150206835\n",
            "Iteration number: 27 Current Objective Value: 0.06587551194035951\n",
            "Iteration number: 28 Current Objective Value: 0.06518687879655277\n",
            "Iteration number: 29 Current Objective Value: 0.06452975447877399\n",
            "Iteration number: 30 Current Objective Value: 0.06390173217147499\n",
            "Iteration number: 31 Current Objective Value: 0.06330065667608556\n",
            "Iteration number: 32 Current Objective Value: 0.0627245915074399\n",
            "Iteration number: 33 Current Objective Value: 0.06217179109960718\n",
            "Iteration number: 34 Current Objective Value: 0.06164067721027574\n",
            "Iteration number: 35 Current Objective Value: 0.061129818794974554\n",
            "Iteration number: 36 Current Objective Value: 0.060637914764362845\n",
            "Iteration number: 37 Current Objective Value: 0.060163779149249356\n",
            "Iteration number: 38 Current Objective Value: 0.05970632828607081\n",
            "Iteration number: 39 Current Objective Value: 0.05926456970560511\n",
            "Iteration number: 40 Current Objective Value: 0.058837592463739805\n",
            "Iteration number: 41 Current Objective Value: 0.05842455869821586\n",
            "Iteration number: 42 Current Objective Value: 0.05802469623174915\n",
            "Iteration number: 43 Current Objective Value: 0.05763729207159913\n",
            "Iteration number: 44 Current Objective Value: 0.05726168667989241\n",
            "Iteration number: 45 Current Objective Value: 0.05689726890890614\n",
            "Iteration number: 46 Current Objective Value: 0.056543471511920976\n",
            "Iteration number: 47 Current Objective Value: 0.05619976715383628\n",
            "Iteration number: 48 Current Objective Value: 0.055865664857032815\n",
            "Iteration number: 49 Current Objective Value: 0.05554070682739155\n",
            "Iteration number: 50 Current Objective Value: 0.05522446561327348\n",
            "Iteration number: 51 Current Objective Value: 0.05491654155690034\n",
            "Iteration number: 52 Current Objective Value: 0.05461656050317688\n",
            "Iteration number: 53 Current Objective Value: 0.054324171735733555\n",
            "Iteration number: 54 Current Objective Value: 0.05403904611399212\n",
            "Iteration number: 55 Current Objective Value: 0.05376087438848451\n",
            "Iteration number: 56 Current Objective Value: 0.053489365674582574\n",
            "Iteration number: 57 Current Objective Value: 0.05322424606730443\n",
            "Iteration number: 58 Current Objective Value: 0.0529652573820179\n",
            "Iteration number: 59 Current Objective Value: 0.05271215600771727\n",
            "Iteration number: 60 Current Objective Value: 0.05246471186115239\n",
            "Iteration number: 61 Current Objective Value: 0.05222270743147674\n",
            "Iteration number: 62 Current Objective Value: 0.051985936906284766\n",
            "Iteration number: 63 Current Objective Value: 0.051754205370956964\n",
            "Iteration number: 64 Current Objective Value: 0.0515273280741436\n",
            "Iteration number: 65 Current Objective Value: 0.05130512975301615\n",
            "Iteration number: 66 Current Objective Value: 0.05108744401261425\n",
            "Iteration number: 67 Current Objective Value: 0.050874112754228706\n",
            "Iteration number: 68 Current Objective Value: 0.05066498564829989\n",
            "Iteration number: 69 Current Objective Value: 0.05045991964778596\n",
            "Iteration number: 70 Current Objective Value: 0.05025877853837381\n",
            "Iteration number: 71 Current Objective Value: 0.050061432522276066\n",
            "Iteration number: 72 Current Objective Value: 0.049867757832685974\n",
            "Iteration number: 73 Current Objective Value: 0.049677636376252374\n",
            "Iteration number: 74 Current Objective Value: 0.04949095540119606\n",
            "Iteration number: 75 Current Objective Value: 0.04930760718891905\n",
            "Iteration number: 76 Current Objective Value: 0.049127488767163406\n",
            "Iteration number: 77 Current Objective Value: 0.04895050164295974\n",
            "Iteration number: 78 Current Objective Value: 0.048776551553769386\n",
            "Iteration number: 79 Current Objective Value: 0.048605548235371\n",
            "Iteration number: 80 Current Objective Value: 0.04843740520517428\n",
            "Iteration number: 81 Current Objective Value: 0.04827203955976159\n",
            "Iteration number: 82 Current Objective Value: 0.04810937178556442\n",
            "Iteration number: 83 Current Objective Value: 0.047949325581678075\n",
            "Iteration number: 84 Current Objective Value: 0.04779182769390377\n",
            "Iteration number: 85 Current Objective Value: 0.047636807759185384\n",
            "Iteration number: 86 Current Objective Value: 0.04748419815967904\n",
            "Iteration number: 87 Current Objective Value: 0.04733393388575683\n",
            "Iteration number: 88 Current Objective Value: 0.04718595240730451\n",
            "Iteration number: 89 Current Objective Value: 0.04704019355272494\n",
            "Iteration number: 90 Current Objective Value: 0.04689659939510672\n",
            "Iteration number: 91 Current Objective Value: 0.046755114145061165\n",
            "Iteration number: 92 Current Objective Value: 0.04661568404976962\n",
            "Iteration number: 93 Current Objective Value: 0.04647825729781952\n",
            "Iteration number: 94 Current Objective Value: 0.04634278392943988\n",
            "Iteration number: 95 Current Objective Value: 0.04620921575177722\n",
            "Iteration number: 96 Current Objective Value: 0.04607750625887995\n",
            "Iteration number: 97 Current Objective Value: 0.04594761055608429\n",
            "Iteration number: 98 Current Objective Value: 0.04581948528851781\n",
            "Iteration number: 99 Current Objective Value: 0.04569308857345737\n",
            "Iteration number: 100 Current Objective Value: 0.04556837993629776\n"
          ]
        }
      ],
      "source": [
        "# Train logistic regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "lam = 0\n",
        "learning_rate = 1\n",
        "w = np.zeros((d,1))\n",
        "w_gd, objvals_gd = gradient_descent(x_train, y_train, lam, learning_rate, w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ8K4S4KH5N-",
        "outputId": "f232eaa0-b7c2-4ef4-87d9-d1438bd6321b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration number: 1 Current Objective Value: 0.6931471805599453\n",
            "Iteration number: 2 Current Objective Value: 0.15922088009733043\n",
            "Iteration number: 3 Current Objective Value: 0.1302606513432094\n",
            "Iteration number: 4 Current Objective Value: 0.11528843378033288\n",
            "Iteration number: 5 Current Objective Value: 0.1066862470757165\n",
            "Iteration number: 6 Current Objective Value: 0.10099964706741345\n",
            "Iteration number: 7 Current Objective Value: 0.09671094652844654\n",
            "Iteration number: 8 Current Objective Value: 0.09320623715306418\n",
            "Iteration number: 9 Current Objective Value: 0.09021811445924562\n",
            "Iteration number: 10 Current Objective Value: 0.08760899793588407\n",
            "Iteration number: 11 Current Objective Value: 0.0852959796503934\n",
            "Iteration number: 12 Current Objective Value: 0.08322305142165068\n",
            "Iteration number: 13 Current Objective Value: 0.08134947738095714\n",
            "Iteration number: 14 Current Objective Value: 0.07964420928468809\n",
            "Iteration number: 15 Current Objective Value: 0.07808284229396174\n",
            "Iteration number: 16 Current Objective Value: 0.07664577471484216\n",
            "Iteration number: 17 Current Objective Value: 0.0753170055748623\n",
            "Iteration number: 18 Current Objective Value: 0.0740833037382127\n",
            "Iteration number: 19 Current Objective Value: 0.07293361033037221\n",
            "Iteration number: 20 Current Objective Value: 0.07185859624359461\n",
            "Iteration number: 21 Current Objective Value: 0.07085032717924226\n",
            "Iteration number: 22 Current Objective Value: 0.06990200565910747\n",
            "Iteration number: 23 Current Objective Value: 0.0690077694804278\n",
            "Iteration number: 24 Current Objective Value: 0.06816253236433582\n",
            "Iteration number: 25 Current Objective Value: 0.06736185664311344\n",
            "Iteration number: 26 Current Objective Value: 0.06660185059842919\n",
            "Iteration number: 27 Current Objective Value: 0.06587908498364976\n",
            "Iteration number: 28 Current Objective Value: 0.06519052462665381\n",
            "Iteration number: 29 Current Objective Value: 0.06453347199486549\n",
            "Iteration number: 30 Current Objective Value: 0.06390552032723197\n",
            "Iteration number: 31 Current Objective Value: 0.06330451447540017\n",
            "Iteration number: 32 Current Objective Value: 0.06272851800059648\n",
            "Iteration number: 33 Current Objective Value: 0.062175785379850544\n",
            "Iteration number: 34 Current Objective Value: 0.06164473841072244\n",
            "Iteration number: 35 Current Objective Value: 0.06113394608582435\n",
            "Iteration number: 36 Current Objective Value: 0.06064210735037413\n",
            "Iteration number: 37 Current Objective Value: 0.060168036267447135\n",
            "Iteration number: 38 Current Objective Value: 0.05971064920366081\n",
            "Iteration number: 39 Current Objective Value: 0.05926895371807052\n",
            "Iteration number: 40 Current Objective Value: 0.05884203889310104\n",
            "Iteration number: 41 Current Objective Value: 0.05842906689143549\n",
            "Iteration number: 42 Current Objective Value: 0.05802926555926722\n",
            "Iteration number: 43 Current Objective Value: 0.05764192192598514\n",
            "Iteration number: 44 Current Objective Value: 0.057266376474602594\n",
            "Iteration number: 45 Current Objective Value: 0.056902018077135574\n",
            "Iteration number: 46 Current Objective Value: 0.056548279505541366\n",
            "Iteration number: 47 Current Objective Value: 0.056204633442411625\n",
            "Iteration number: 48 Current Objective Value: 0.055870588926905405\n",
            "Iteration number: 49 Current Objective Value: 0.055545688180832414\n",
            "Iteration number: 50 Current Objective Value: 0.055229503767691084\n",
            "Iteration number: 51 Current Objective Value: 0.054921636044102856\n",
            "Iteration number: 52 Current Objective Value: 0.05462171086868334\n",
            "Iteration number: 53 Current Objective Value: 0.054329377538129685\n",
            "Iteration number: 54 Current Objective Value: 0.05404430692432741\n",
            "Iteration number: 55 Current Objective Value: 0.053766189789707\n",
            "Iteration number: 56 Current Objective Value: 0.05349473526100853\n",
            "Iteration number: 57 Current Objective Value: 0.05322966944412001\n",
            "Iteration number: 58 Current Objective Value: 0.052970734164810566\n",
            "Iteration number: 59 Current Objective Value: 0.052717685822034545\n",
            "Iteration number: 60 Current Objective Value: 0.052470294342086055\n",
            "Iteration number: 61 Current Objective Value: 0.05222834222327049\n",
            "Iteration number: 62 Current Objective Value: 0.051991623661963846\n",
            "Iteration number: 63 Current Objective Value: 0.051759943751978024\n",
            "Iteration number: 64 Current Objective Value: 0.051533117750063466\n",
            "Iteration number: 65 Current Objective Value: 0.05131097040117823\n",
            "Iteration number: 66 Current Objective Value: 0.051093335317851415\n",
            "Iteration number: 67 Current Objective Value: 0.05088005440858134\n",
            "Iteration number: 68 Current Objective Value: 0.05067097735074853\n",
            "Iteration number: 69 Current Objective Value: 0.05046596110399717\n",
            "Iteration number: 70 Current Objective Value: 0.050264869460458626\n",
            "Iteration number: 71 Current Objective Value: 0.050067572628560326\n",
            "Iteration number: 72 Current Objective Value: 0.04987394684749162\n",
            "Iteration number: 73 Current Objective Value: 0.04968387402968911\n",
            "Iteration number: 74 Current Objective Value: 0.04949724142896285\n",
            "Iteration number: 75 Current Objective Value: 0.04931394133211466\n",
            "Iteration number: 76 Current Objective Value: 0.04913387077210575\n",
            "Iteration number: 77 Current Objective Value: 0.04895693126101325\n",
            "Iteration number: 78 Current Objective Value: 0.04878302854118002\n",
            "Iteration number: 79 Current Objective Value: 0.04861207235310864\n",
            "Iteration number: 80 Current Objective Value: 0.04844397621878188\n",
            "Iteration number: 81 Current Objective Value: 0.04827865723921068\n",
            "Iteration number: 82 Current Objective Value: 0.048116035905116955\n",
            "Iteration number: 83 Current Objective Value: 0.047956035919753906\n",
            "Iteration number: 84 Current Objective Value: 0.04779858403295358\n",
            "Iteration number: 85 Current Objective Value: 0.04764360988556889\n",
            "Iteration number: 86 Current Objective Value: 0.04749104586354808\n",
            "Iteration number: 87 Current Objective Value: 0.04734082696094312\n",
            "Iteration number: 88 Current Objective Value: 0.0471928906512118\n",
            "Iteration number: 89 Current Objective Value: 0.047047176766225426\n",
            "Iteration number: 90 Current Objective Value: 0.04690362738244142\n",
            "Iteration number: 91 Current Objective Value: 0.04676218671374412\n",
            "Iteration number: 92 Current Objective Value: 0.046622801010495686\n",
            "Iteration number: 93 Current Objective Value: 0.0464854184643756\n",
            "Iteration number: 94 Current Objective Value: 0.04634998911861951\n",
            "Iteration number: 95 Current Objective Value: 0.046216464783298127\n",
            "Iteration number: 96 Current Objective Value: 0.04608479895530475\n",
            "Iteration number: 97 Current Objective Value: 0.04595494674274394\n",
            "Iteration number: 98 Current Objective Value: 0.04582686479343778\n",
            "Iteration number: 99 Current Objective Value: 0.04570051122728643\n",
            "Iteration number: 100 Current Objective Value: 0.04557584557223918\n"
          ]
        }
      ],
      "source": [
        "# Train regularized logistric regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "lam = 0.000001\n",
        "learning_rate = 1\n",
        "w = np.zeros((d,1))\n",
        "w_gd_r, objvals_gdr = gradient_descent(x_train, y_train, lam, learning_rate, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvVw5thDH5N-"
      },
      "source": [
        "## 3.2. Stochastic gradient descent (SGD)\n",
        "\n",
        "Define new objective function $Q_i (w) = \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $. \n",
        "\n",
        "The stochastic gradient at $w$ is $g_i = \\frac{\\partial Q_i }{ \\partial w} = -\\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
        "\n",
        "You may need to implement a new function to calculate the new objective function and gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {
        "id": "bwnt1QM0H5N-"
      },
      "outputs": [],
      "source": [
        "# Calculate the objective Q_i and the gradient of Q_i\n",
        "# Inputs:\n",
        "#     w: weights: d-by-1 matrix\n",
        "#     xi: data: 1-by-d matrix\n",
        "#     yi: label: scalar\n",
        "#     lam: scalar, the regularization parameter\n",
        "# Return:\n",
        "#     obj: scalar, the objective Q_i\n",
        "#     g: d-by-1 matrix, gradient of Q_i\n",
        "\n",
        "def stochastic_objective_gradient(w, xi, yi, lam):\n",
        "  \n",
        "  yixi = yi * xi\n",
        "  yixiw = float(np.dot(yixi,w))\n",
        "  \n",
        "  firstObj = np.log(1 + np.exp(-yixiw))\n",
        "  secondObj = (lam / 2) * np.sum(w * w)\n",
        "  \n",
        "  obj = firstObj + secondObj\n",
        "  \n",
        "  firstG = -yixi.T/(1 + np.exp(yixiw))\n",
        "  secondG = lam * w\n",
        "  \n",
        "  g = firstG + secondG\n",
        "  return obj, g\n",
        "  \n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ6MopoQH5N_"
      },
      "source": [
        "Hints:\n",
        "1. In every epoch, randomly permute the $n$ samples.\n",
        "2. Each epoch has $n$ iterations. In every iteration, use 1 sample, and compute the gradient and objective using the ``stochastic_objective_gradient`` function. In the next iteration, use the next sample, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "id": "MCXZ4M0hH5N_"
      },
      "outputs": [],
      "source": [
        "# SGD for solving logistic regression\n",
        "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
        "\n",
        "# Inputs:\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: scalar, the regularization parameter\n",
        "#     learning_rate: scalar\n",
        "#     w: weights: d-by-1 matrix, initialization of w\n",
        "#     max_epoch: integer, the maximal epochs\n",
        "# Return:\n",
        "#     \n",
        "#     w: weights: d-by-1 matrix, the solution\n",
        "#     objvals: a record of each epoch's objective value\n",
        "#     Record one objective value per epoch (not per iteration)\n",
        "\n",
        "def sgd(x, y, lam, learning_rate, w, max_epoch=100):\n",
        "\n",
        "  n = x.shape[0]\n",
        "  objvals = np.zeros(max_epoch)\n",
        "\n",
        "  for i in range(0, max_epoch):\n",
        "\n",
        "    # shuffling \n",
        "    randomIndices = np.random.permutation(n)\n",
        "    x_random, y_random = x[randomIndices, :], y[randomIndices, :]\n",
        "\n",
        "    currentObjValue = 0\n",
        "    for j in range(0,n):\n",
        "      xi, yi = x_random[j, :].reshape(1,d), float(y_random[j, :])\n",
        "      currObj, g = stochastic_objective_gradient(w, xi, yi, lam)\n",
        "      w -= learning_rate * g # update weights\n",
        "      currentObjValue += currObj\n",
        "    \n",
        "    learning_rate *= 0.95\n",
        "    currentObjValue /= n\n",
        "    objvals[i] = currentObjValue\n",
        "    print(f'Iteration number: {i+1} Current Objective Value: {objvals[i]}')\n",
        "\n",
        "  return w, objvals\n",
        "\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-YHVGh1H5N_"
      },
      "source": [
        "Use sgd function to obtain your optimal weights and a list of objective values over each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEKAMh2RH5N_",
        "outputId": "4747e3d5-69ee-48b5-e4aa-b7a4adb7b795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration number: 1 Current Objective Value: 0.22150518484811352\n",
            "Iteration number: 2 Current Objective Value: 0.38254717271398325\n",
            "Iteration number: 3 Current Objective Value: 0.3485947125644403\n",
            "Iteration number: 4 Current Objective Value: 0.14141274882783347\n",
            "Iteration number: 5 Current Objective Value: 0.1498623066125381\n",
            "Iteration number: 6 Current Objective Value: 0.1590234991325371\n",
            "Iteration number: 7 Current Objective Value: 0.16310772112875233\n",
            "Iteration number: 8 Current Objective Value: 0.09170321249725709\n",
            "Iteration number: 9 Current Objective Value: 0.2629114317191411\n",
            "Iteration number: 10 Current Objective Value: 0.10321917571166685\n",
            "Iteration number: 11 Current Objective Value: 0.07902071575365872\n",
            "Iteration number: 12 Current Objective Value: 0.0656712284684685\n",
            "Iteration number: 13 Current Objective Value: 0.07752595671838158\n",
            "Iteration number: 14 Current Objective Value: 0.05882552247158761\n",
            "Iteration number: 15 Current Objective Value: 0.04710488744988399\n",
            "Iteration number: 16 Current Objective Value: 0.04037821857841138\n",
            "Iteration number: 17 Current Objective Value: 0.07082143641939588\n",
            "Iteration number: 18 Current Objective Value: 0.04211187736522238\n",
            "Iteration number: 19 Current Objective Value: 0.05375978507280174\n",
            "Iteration number: 20 Current Objective Value: 0.04744869160438471\n",
            "Iteration number: 21 Current Objective Value: 0.06343560756135887\n",
            "Iteration number: 22 Current Objective Value: 0.04881870798682151\n",
            "Iteration number: 23 Current Objective Value: 0.03459462350974672\n",
            "Iteration number: 24 Current Objective Value: 0.02667214538802318\n",
            "Iteration number: 25 Current Objective Value: 0.022686308264737932\n",
            "Iteration number: 26 Current Objective Value: 0.0255801928169431\n",
            "Iteration number: 27 Current Objective Value: 0.027073840467969457\n",
            "Iteration number: 28 Current Objective Value: 0.030214941178174356\n",
            "Iteration number: 29 Current Objective Value: 0.02325010621484713\n",
            "Iteration number: 30 Current Objective Value: 0.02115571824735266\n",
            "Iteration number: 31 Current Objective Value: 0.019682869620672446\n",
            "Iteration number: 32 Current Objective Value: 0.018217887008502798\n",
            "Iteration number: 33 Current Objective Value: 0.017996142999888252\n",
            "Iteration number: 34 Current Objective Value: 0.02177308463534627\n",
            "Iteration number: 35 Current Objective Value: 0.017790389742289872\n",
            "Iteration number: 36 Current Objective Value: 0.01627923256326663\n",
            "Iteration number: 37 Current Objective Value: 0.015971503619767968\n",
            "Iteration number: 38 Current Objective Value: 0.016020670448264905\n",
            "Iteration number: 39 Current Objective Value: 0.01571302813118742\n",
            "Iteration number: 40 Current Objective Value: 0.015642722893467464\n",
            "Iteration number: 41 Current Objective Value: 0.015480866122428185\n",
            "Iteration number: 42 Current Objective Value: 0.015080409871307907\n",
            "Iteration number: 43 Current Objective Value: 0.01449798276416838\n",
            "Iteration number: 44 Current Objective Value: 0.014586105087283512\n",
            "Iteration number: 45 Current Objective Value: 0.014511442611558126\n",
            "Iteration number: 46 Current Objective Value: 0.014319483538038308\n",
            "Iteration number: 47 Current Objective Value: 0.014069989749623283\n",
            "Iteration number: 48 Current Objective Value: 0.013822872344979783\n",
            "Iteration number: 49 Current Objective Value: 0.013745213175787765\n",
            "Iteration number: 50 Current Objective Value: 0.013686854315246902\n",
            "Iteration number: 51 Current Objective Value: 0.013703857669849136\n",
            "Iteration number: 52 Current Objective Value: 0.013475762523435053\n",
            "Iteration number: 53 Current Objective Value: 0.013325133066759096\n",
            "Iteration number: 54 Current Objective Value: 0.013196966302471465\n",
            "Iteration number: 55 Current Objective Value: 0.01306189380474392\n",
            "Iteration number: 56 Current Objective Value: 0.013024536810589495\n",
            "Iteration number: 57 Current Objective Value: 0.012949048888992893\n",
            "Iteration number: 58 Current Objective Value: 0.012885073362225198\n",
            "Iteration number: 59 Current Objective Value: 0.012794791363317225\n",
            "Iteration number: 60 Current Objective Value: 0.012704029605610335\n",
            "Iteration number: 61 Current Objective Value: 0.012637686467525059\n",
            "Iteration number: 62 Current Objective Value: 0.012607674254901753\n",
            "Iteration number: 63 Current Objective Value: 0.012508092957805014\n",
            "Iteration number: 64 Current Objective Value: 0.012484549603126949\n",
            "Iteration number: 65 Current Objective Value: 0.012428012751594542\n",
            "Iteration number: 66 Current Objective Value: 0.012360291571418712\n",
            "Iteration number: 67 Current Objective Value: 0.012305408030561758\n",
            "Iteration number: 68 Current Objective Value: 0.012265510272312126\n",
            "Iteration number: 69 Current Objective Value: 0.012225921511260245\n",
            "Iteration number: 70 Current Objective Value: 0.01217379435350616\n",
            "Iteration number: 71 Current Objective Value: 0.012139574120430409\n",
            "Iteration number: 72 Current Objective Value: 0.012114381168010958\n",
            "Iteration number: 73 Current Objective Value: 0.012071181549649732\n",
            "Iteration number: 74 Current Objective Value: 0.012044795950644751\n",
            "Iteration number: 75 Current Objective Value: 0.01201379224573302\n",
            "Iteration number: 76 Current Objective Value: 0.011979176880283998\n",
            "Iteration number: 77 Current Objective Value: 0.01194914710396122\n",
            "Iteration number: 78 Current Objective Value: 0.011924542641323446\n",
            "Iteration number: 79 Current Objective Value: 0.01190143330770979\n",
            "Iteration number: 80 Current Objective Value: 0.011878022298627108\n",
            "Iteration number: 81 Current Objective Value: 0.011857104400357745\n",
            "Iteration number: 82 Current Objective Value: 0.01183489815352698\n",
            "Iteration number: 83 Current Objective Value: 0.011814154479574495\n",
            "Iteration number: 84 Current Objective Value: 0.011796468120614422\n",
            "Iteration number: 85 Current Objective Value: 0.01177767211442615\n",
            "Iteration number: 86 Current Objective Value: 0.011757782361406652\n",
            "Iteration number: 87 Current Objective Value: 0.011740676573837757\n",
            "Iteration number: 88 Current Objective Value: 0.011726649467283797\n",
            "Iteration number: 89 Current Objective Value: 0.011713441832278529\n",
            "Iteration number: 90 Current Objective Value: 0.01169923389608821\n",
            "Iteration number: 91 Current Objective Value: 0.01168539880009382\n",
            "Iteration number: 92 Current Objective Value: 0.011673081900703718\n",
            "Iteration number: 93 Current Objective Value: 0.011660888531498827\n",
            "Iteration number: 94 Current Objective Value: 0.011648147106205053\n",
            "Iteration number: 95 Current Objective Value: 0.01163847431329198\n",
            "Iteration number: 96 Current Objective Value: 0.011628734769886703\n",
            "Iteration number: 97 Current Objective Value: 0.011619402016307696\n",
            "Iteration number: 98 Current Objective Value: 0.011610355072521414\n",
            "Iteration number: 99 Current Objective Value: 0.01160088558484212\n",
            "Iteration number: 100 Current Objective Value: 0.01159317161817787\n"
          ]
        }
      ],
      "source": [
        "# Train logistic regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "lam = 0\n",
        "learning_rate = 1\n",
        "w = np.zeros((d,1))\n",
        "w_sgd, objvals_sgd = sgd(x_train, y_train, lam, learning_rate, w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {
        "id": "J4CbqJUKH5OA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd22a1af-f9c9-4083-f6ab-b41579665af5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration number: 1 Current Objective Value: 0.22535028746919758\n",
            "Iteration number: 2 Current Objective Value: 0.1685584519594464\n",
            "Iteration number: 3 Current Objective Value: 0.2513660599343902\n",
            "Iteration number: 4 Current Objective Value: 0.22536506061971814\n",
            "Iteration number: 5 Current Objective Value: 0.1956332201283321\n",
            "Iteration number: 6 Current Objective Value: 0.1783203179653239\n",
            "Iteration number: 7 Current Objective Value: 0.11214969310945126\n",
            "Iteration number: 8 Current Objective Value: 0.13802779846227578\n",
            "Iteration number: 9 Current Objective Value: 0.09364153047603536\n",
            "Iteration number: 10 Current Objective Value: 0.07484995045540913\n",
            "Iteration number: 11 Current Objective Value: 0.13630157719505223\n",
            "Iteration number: 12 Current Objective Value: 0.1746075498124296\n",
            "Iteration number: 13 Current Objective Value: 0.1131554014412709\n",
            "Iteration number: 14 Current Objective Value: 0.09063116568993654\n",
            "Iteration number: 15 Current Objective Value: 0.04714330669353583\n",
            "Iteration number: 16 Current Objective Value: 0.1647252055375864\n",
            "Iteration number: 17 Current Objective Value: 0.055575141370527326\n",
            "Iteration number: 18 Current Objective Value: 0.049753135233374726\n",
            "Iteration number: 19 Current Objective Value: 0.05895544246170919\n",
            "Iteration number: 20 Current Objective Value: 0.05991187377423145\n",
            "Iteration number: 21 Current Objective Value: 0.03778427216467998\n",
            "Iteration number: 22 Current Objective Value: 0.06130342222696962\n",
            "Iteration number: 23 Current Objective Value: 0.030179774602936246\n",
            "Iteration number: 24 Current Objective Value: 0.02900368473940186\n",
            "Iteration number: 25 Current Objective Value: 0.026136529272111245\n",
            "Iteration number: 26 Current Objective Value: 0.02248651544789725\n",
            "Iteration number: 27 Current Objective Value: 0.0251348936608979\n",
            "Iteration number: 28 Current Objective Value: 0.028830276827604986\n",
            "Iteration number: 29 Current Objective Value: 0.028205391085921787\n",
            "Iteration number: 30 Current Objective Value: 0.027482758241170923\n",
            "Iteration number: 31 Current Objective Value: 0.021398502084409757\n",
            "Iteration number: 32 Current Objective Value: 0.020932259039595284\n",
            "Iteration number: 33 Current Objective Value: 0.02113266170309942\n",
            "Iteration number: 34 Current Objective Value: 0.025574302652634176\n",
            "Iteration number: 35 Current Objective Value: 0.018963930292804944\n",
            "Iteration number: 36 Current Objective Value: 0.0169059008050709\n",
            "Iteration number: 37 Current Objective Value: 0.0176991147899487\n",
            "Iteration number: 38 Current Objective Value: 0.017821731218231426\n",
            "Iteration number: 39 Current Objective Value: 0.017016299614980433\n",
            "Iteration number: 40 Current Objective Value: 0.016049183992830427\n",
            "Iteration number: 41 Current Objective Value: 0.01656990631520189\n",
            "Iteration number: 42 Current Objective Value: 0.017305322402229144\n",
            "Iteration number: 43 Current Objective Value: 0.015545500652391608\n",
            "Iteration number: 44 Current Objective Value: 0.016514124453138976\n",
            "Iteration number: 45 Current Objective Value: 0.015681290675815123\n",
            "Iteration number: 46 Current Objective Value: 0.014998932787730752\n",
            "Iteration number: 47 Current Objective Value: 0.014780444311266192\n",
            "Iteration number: 48 Current Objective Value: 0.01461650042324834\n",
            "Iteration number: 49 Current Objective Value: 0.014622643970210417\n",
            "Iteration number: 50 Current Objective Value: 0.014316437032410676\n",
            "Iteration number: 51 Current Objective Value: 0.014254304004376798\n",
            "Iteration number: 52 Current Objective Value: 0.01412347006089138\n",
            "Iteration number: 53 Current Objective Value: 0.013897143817363032\n",
            "Iteration number: 54 Current Objective Value: 0.013927405062346071\n",
            "Iteration number: 55 Current Objective Value: 0.013746320012156856\n",
            "Iteration number: 56 Current Objective Value: 0.013638448802366615\n",
            "Iteration number: 57 Current Objective Value: 0.013619419104274968\n",
            "Iteration number: 58 Current Objective Value: 0.013476685302077775\n",
            "Iteration number: 59 Current Objective Value: 0.013427808712496107\n",
            "Iteration number: 60 Current Objective Value: 0.013336560409458827\n",
            "Iteration number: 61 Current Objective Value: 0.013274356639073866\n",
            "Iteration number: 62 Current Objective Value: 0.013150731260397815\n",
            "Iteration number: 63 Current Objective Value: 0.013168498099911571\n",
            "Iteration number: 64 Current Objective Value: 0.013103900561315242\n",
            "Iteration number: 65 Current Objective Value: 0.012997609877869425\n",
            "Iteration number: 66 Current Objective Value: 0.012977919121420517\n",
            "Iteration number: 67 Current Objective Value: 0.012919296847629378\n",
            "Iteration number: 68 Current Objective Value: 0.012877972520047298\n",
            "Iteration number: 69 Current Objective Value: 0.01282463358176723\n",
            "Iteration number: 70 Current Objective Value: 0.012764033653668901\n",
            "Iteration number: 71 Current Objective Value: 0.012735128817768672\n",
            "Iteration number: 72 Current Objective Value: 0.012706933568758008\n",
            "Iteration number: 73 Current Objective Value: 0.012670151676877411\n",
            "Iteration number: 74 Current Objective Value: 0.012634403613723123\n",
            "Iteration number: 75 Current Objective Value: 0.012597278873841409\n",
            "Iteration number: 76 Current Objective Value: 0.012564235326114084\n",
            "Iteration number: 77 Current Objective Value: 0.012540711256254288\n",
            "Iteration number: 78 Current Objective Value: 0.01251728371557265\n",
            "Iteration number: 79 Current Objective Value: 0.012484504464232103\n",
            "Iteration number: 80 Current Objective Value: 0.012462580942602847\n",
            "Iteration number: 81 Current Objective Value: 0.012438740509387673\n",
            "Iteration number: 82 Current Objective Value: 0.012418503265890724\n",
            "Iteration number: 83 Current Objective Value: 0.012398577536068144\n",
            "Iteration number: 84 Current Objective Value: 0.012368325040678294\n",
            "Iteration number: 85 Current Objective Value: 0.012360539022871776\n",
            "Iteration number: 86 Current Objective Value: 0.012343305186647079\n",
            "Iteration number: 87 Current Objective Value: 0.012322305867531252\n",
            "Iteration number: 88 Current Objective Value: 0.012308306788144591\n",
            "Iteration number: 89 Current Objective Value: 0.012291730331732779\n",
            "Iteration number: 90 Current Objective Value: 0.012277262451112735\n",
            "Iteration number: 91 Current Objective Value: 0.012264466235028072\n",
            "Iteration number: 92 Current Objective Value: 0.012250323339411602\n",
            "Iteration number: 93 Current Objective Value: 0.012239449197070148\n",
            "Iteration number: 94 Current Objective Value: 0.012226634034844698\n",
            "Iteration number: 95 Current Objective Value: 0.012216370365694031\n",
            "Iteration number: 96 Current Objective Value: 0.012205408040296186\n",
            "Iteration number: 97 Current Objective Value: 0.012193981976742043\n",
            "Iteration number: 98 Current Objective Value: 0.0121858045059149\n",
            "Iteration number: 99 Current Objective Value: 0.012176313464464983\n",
            "Iteration number: 100 Current Objective Value: 0.012166465651925655\n"
          ]
        }
      ],
      "source": [
        "# Train regularized logistric regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "lam = 0.000001\n",
        "learning_rate = 1\n",
        "w = np.zeros((x_train.shape[1],1))\n",
        "w_sgd_r, objvals_sgdr = sgd(x_train, y_train, lam, learning_rate, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvnUxiWcH5OA"
      },
      "source": [
        "## 3.3 Mini-Batch Gradient Descent (MBGD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAgfrS7aH5OA"
      },
      "source": [
        "Define $Q_I (w) = \\frac{1}{b} \\sum_{i \\in I} \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $, where $I$ is a set containing $b$ indices randomly drawn from $\\{ 1, \\cdots , n \\}$ without replacement.\n",
        "\n",
        "The stochastic gradient at $w$ is $g_I = \\frac{\\partial Q_I }{ \\partial w} = \\frac{1}{b} \\sum_{i \\in I} \\frac{- y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
        "\n",
        "You may need to implement a new function to calculate the new objective function and gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {
        "id": "-9apqlgBH5OA"
      },
      "outputs": [],
      "source": [
        "# Calculate the objective Q_I and the gradient of Q_I\n",
        "# Inputs:\n",
        "#     w: weights: d-by-b matrix\n",
        "#     xi: data: b-by-d matrix\n",
        "#     yi: label: scalar\n",
        "#     lam: scalar, the regularization parameter\n",
        "# Return:\n",
        "#     obj: scalar, the objective Q_i\n",
        "#     g: d-by-1 matrix, gradient of Q_i\n",
        "\n",
        "def mb_objective_gradient(w, xi, yi, lam):\n",
        "\n",
        "  yixi = np.multiply(yi, xi)\n",
        "  yixiw = np.dot(yixi, w)\n",
        "  \n",
        "  firstObj = np.mean(np.log(1 + np.exp(-yixiw)))\n",
        "  secondObj = (lam/2) * np.sum(w * w)\n",
        "  \n",
        "  obj = firstObj + secondObj\n",
        "  \n",
        "  firstG = np.mean(np.divide(-yixi, 1 + np.exp(yixiw)), axis = 0).reshape(d,1)\n",
        "  secondG = lam * w\n",
        "  \n",
        "  g = firstG + secondG\n",
        "  \n",
        "  return obj, g\n",
        "  \n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9I_yvPZH5OA"
      },
      "source": [
        "Hints:\n",
        "1. In every epoch, randomly permute the $n$ samples (just like SGD).\n",
        "2. Each epoch has $\\frac{n}{b}$ iterations. In every iteration, use $b$ samples, and compute the gradient and objective using the ``mb_objective_gradient`` function. In the next iteration, use the next $b$ samples, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "id": "-F9aQKVAH5OA"
      },
      "outputs": [],
      "source": [
        "# MBGD for solving logistic regression\n",
        "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
        "\n",
        "# Inputs:\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: scalar, the regularization parameter\n",
        "#     learning_rate: scalar\n",
        "#     w: weights: d-by-1 matrix, initialization of w\n",
        "#     max_epoch: integer, the maximal epochs\n",
        "# Return:\n",
        "#     w: weights: d-by-1 matrix, the solution\n",
        "#     objvals: a record of each epoch's objective value\n",
        "#     Record one objective value per epoch (not per iteration)\n",
        "\n",
        "def mbgd(x, y, lam, learning_rate, w, max_epoch=100):\n",
        "\n",
        "  objvals = np.zeros(max_epoch)\n",
        "  batchSize = 10\n",
        "  n = x.shape[0]\n",
        "\n",
        "  for i in range(max_epoch) :\n",
        "    \n",
        "    randomIndices = np.random.permutation(n)\n",
        "    x_random, y_random = x[randomIndices, : ], y [randomIndices, : ]\n",
        "    \n",
        "    currentObjValue = 0\n",
        "    for j in range(0, n, batchSize):\n",
        "      xi, yi = x_random[j : j + batchSize, :], y_random[j : j + batchSize, :]\n",
        "      currObj, g = mb_objective_gradient(w, xi, yi, lam)\n",
        "      currentObjValue += currObj\n",
        "      w -= learning_rate * g\n",
        "\n",
        "    learning_rate *= 0.95\n",
        "    currentObjValue /= (n/batchSize)\n",
        "    objvals[i] = currentObjValue\n",
        "    print(f'Iteration number: {i+1} Current Objective Value: {objvals[i]}')\n",
        "\n",
        "  \n",
        "  return w, objvals\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFEG2XSqH5OA"
      },
      "source": [
        "Use mbgd function to obtain your optimal weights and a list of objective values over each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "id": "wBM-nrcHH5OB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3006641f-a400-4953-8b0c-463e8c63956f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration number: 1 Current Objective Value: 0.10559619814194841\n",
            "Iteration number: 2 Current Objective Value: 0.05774032268790222\n",
            "Iteration number: 3 Current Objective Value: 0.0478817478554882\n",
            "Iteration number: 4 Current Objective Value: 0.049866984307217595\n",
            "Iteration number: 5 Current Objective Value: 0.0442099395307481\n",
            "Iteration number: 6 Current Objective Value: 0.04110669279872025\n",
            "Iteration number: 7 Current Objective Value: 0.038179244887215594\n",
            "Iteration number: 8 Current Objective Value: 0.03748770473663607\n",
            "Iteration number: 9 Current Objective Value: 0.036536233276370796\n",
            "Iteration number: 10 Current Objective Value: 0.03601660547780158\n",
            "Iteration number: 11 Current Objective Value: 0.0342176949398835\n",
            "Iteration number: 12 Current Objective Value: 0.04030353866080866\n",
            "Iteration number: 13 Current Objective Value: 0.03506180266246743\n",
            "Iteration number: 14 Current Objective Value: 0.03313626738677448\n",
            "Iteration number: 15 Current Objective Value: 0.037961620517097616\n",
            "Iteration number: 16 Current Objective Value: 0.03215582212143538\n",
            "Iteration number: 17 Current Objective Value: 0.03271876679132818\n",
            "Iteration number: 18 Current Objective Value: 0.031453471783486966\n",
            "Iteration number: 19 Current Objective Value: 0.03138483638530581\n",
            "Iteration number: 20 Current Objective Value: 0.03177148286016835\n",
            "Iteration number: 21 Current Objective Value: 0.030466467634479903\n",
            "Iteration number: 22 Current Objective Value: 0.032302441098625144\n",
            "Iteration number: 23 Current Objective Value: 0.030152552358201558\n",
            "Iteration number: 24 Current Objective Value: 0.03012853627777299\n",
            "Iteration number: 25 Current Objective Value: 0.030580479541729585\n",
            "Iteration number: 26 Current Objective Value: 0.02936748466027992\n",
            "Iteration number: 27 Current Objective Value: 0.029879861657480054\n",
            "Iteration number: 28 Current Objective Value: 0.028965993216302267\n",
            "Iteration number: 29 Current Objective Value: 0.02885727962783604\n",
            "Iteration number: 30 Current Objective Value: 0.029089600394072095\n",
            "Iteration number: 31 Current Objective Value: 0.034162650839295386\n",
            "Iteration number: 32 Current Objective Value: 0.02840352049569167\n",
            "Iteration number: 33 Current Objective Value: 0.02832778299871822\n",
            "Iteration number: 34 Current Objective Value: 0.028488012117241987\n",
            "Iteration number: 35 Current Objective Value: 0.02901157013753553\n",
            "Iteration number: 36 Current Objective Value: 0.028556634190832028\n",
            "Iteration number: 37 Current Objective Value: 0.02841260680965527\n",
            "Iteration number: 38 Current Objective Value: 0.027732117007555776\n",
            "Iteration number: 39 Current Objective Value: 0.02966484848611101\n",
            "Iteration number: 40 Current Objective Value: 0.028004041498591505\n",
            "Iteration number: 41 Current Objective Value: 0.027486049677392713\n",
            "Iteration number: 42 Current Objective Value: 0.027494943659042224\n",
            "Iteration number: 43 Current Objective Value: 0.027373289397113927\n",
            "Iteration number: 44 Current Objective Value: 0.02757955574445974\n",
            "Iteration number: 45 Current Objective Value: 0.027202611295962627\n",
            "Iteration number: 46 Current Objective Value: 0.027283221414586823\n",
            "Iteration number: 47 Current Objective Value: 0.027098404080008274\n",
            "Iteration number: 48 Current Objective Value: 0.02702625942980261\n",
            "Iteration number: 49 Current Objective Value: 0.027103499551880385\n",
            "Iteration number: 50 Current Objective Value: 0.02697953856425522\n",
            "Iteration number: 51 Current Objective Value: 0.02729561692563647\n",
            "Iteration number: 52 Current Objective Value: 0.02725734732674863\n",
            "Iteration number: 53 Current Objective Value: 0.028282062934053994\n",
            "Iteration number: 54 Current Objective Value: 0.02682098325360463\n",
            "Iteration number: 55 Current Objective Value: 0.026785454555467998\n",
            "Iteration number: 56 Current Objective Value: 0.02729295878187749\n",
            "Iteration number: 57 Current Objective Value: 0.026851861374496435\n",
            "Iteration number: 58 Current Objective Value: 0.026719652598489248\n",
            "Iteration number: 59 Current Objective Value: 0.03321171725703366\n",
            "Iteration number: 60 Current Objective Value: 0.02694941730396963\n",
            "Iteration number: 61 Current Objective Value: 0.02660978325988264\n",
            "Iteration number: 62 Current Objective Value: 0.026700896810705345\n",
            "Iteration number: 63 Current Objective Value: 0.026613000808302607\n",
            "Iteration number: 64 Current Objective Value: 0.02688045398224425\n",
            "Iteration number: 65 Current Objective Value: 0.02826386447324513\n",
            "Iteration number: 66 Current Objective Value: 0.02653316666869502\n",
            "Iteration number: 67 Current Objective Value: 0.026522080724108832\n",
            "Iteration number: 68 Current Objective Value: 0.026477534632696583\n",
            "Iteration number: 69 Current Objective Value: 0.026590800864714374\n",
            "Iteration number: 70 Current Objective Value: 0.027036300092574534\n",
            "Iteration number: 71 Current Objective Value: 0.026446626972989534\n",
            "Iteration number: 72 Current Objective Value: 0.02643302286350438\n",
            "Iteration number: 73 Current Objective Value: 0.026440065243008456\n",
            "Iteration number: 74 Current Objective Value: 0.02638447239595479\n",
            "Iteration number: 75 Current Objective Value: 0.026742164148608392\n",
            "Iteration number: 76 Current Objective Value: 0.026365633721653785\n",
            "Iteration number: 77 Current Objective Value: 0.026350612058938315\n",
            "Iteration number: 78 Current Objective Value: 0.026344932267768927\n",
            "Iteration number: 79 Current Objective Value: 0.026778522013727848\n",
            "Iteration number: 80 Current Objective Value: 0.026323337940854247\n",
            "Iteration number: 81 Current Objective Value: 0.026555597980363832\n",
            "Iteration number: 82 Current Objective Value: 0.027186359297197323\n",
            "Iteration number: 83 Current Objective Value: 0.026298564463990707\n",
            "Iteration number: 84 Current Objective Value: 0.026327314305639303\n",
            "Iteration number: 85 Current Objective Value: 0.026544649804404706\n",
            "Iteration number: 86 Current Objective Value: 0.02627254207225747\n",
            "Iteration number: 87 Current Objective Value: 0.02648611745070737\n",
            "Iteration number: 88 Current Objective Value: 0.02662602117031453\n",
            "Iteration number: 89 Current Objective Value: 0.02655521838224538\n",
            "Iteration number: 90 Current Objective Value: 0.02636581056778666\n",
            "Iteration number: 91 Current Objective Value: 0.026240006044557194\n",
            "Iteration number: 92 Current Objective Value: 0.026230786111461964\n",
            "Iteration number: 93 Current Objective Value: 0.02622800457018041\n",
            "Iteration number: 94 Current Objective Value: 0.026483549478193133\n",
            "Iteration number: 95 Current Objective Value: 0.02622006101740251\n",
            "Iteration number: 96 Current Objective Value: 0.02622741180198515\n",
            "Iteration number: 97 Current Objective Value: 0.026215223087210207\n",
            "Iteration number: 98 Current Objective Value: 0.027376704089423158\n",
            "Iteration number: 99 Current Objective Value: 0.02622070157611154\n",
            "Iteration number: 100 Current Objective Value: 0.02628402404991491\n"
          ]
        }
      ],
      "source": [
        "# Train logistic regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "lam = 0\n",
        "learning_rate = 1\n",
        "w = np.zeros((x_train.shape[1],1))\n",
        "w_mbgd, objvals_mbgd = mbgd(x_train, y_train, lam, learning_rate, w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "id": "os1wIVUUH5OB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c546de1c-4838-4d2a-f860-a539ff8ca919"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration number: 1 Current Objective Value: 0.11019934634087272\n",
            "Iteration number: 2 Current Objective Value: 0.060458974791153225\n",
            "Iteration number: 3 Current Objective Value: 0.05104932709148299\n",
            "Iteration number: 4 Current Objective Value: 0.04611036363048694\n",
            "Iteration number: 5 Current Objective Value: 0.04413043145607371\n",
            "Iteration number: 6 Current Objective Value: 0.041434520855240554\n",
            "Iteration number: 7 Current Objective Value: 0.04010643019298456\n",
            "Iteration number: 8 Current Objective Value: 0.040944312828033125\n",
            "Iteration number: 9 Current Objective Value: 0.039187791885112636\n",
            "Iteration number: 10 Current Objective Value: 0.03516103199959138\n",
            "Iteration number: 11 Current Objective Value: 0.035585192717827924\n",
            "Iteration number: 12 Current Objective Value: 0.034002065553226\n",
            "Iteration number: 13 Current Objective Value: 0.033435951994100266\n",
            "Iteration number: 14 Current Objective Value: 0.03353642801317132\n",
            "Iteration number: 15 Current Objective Value: 0.03523312417425682\n",
            "Iteration number: 16 Current Objective Value: 0.033047517646905264\n",
            "Iteration number: 17 Current Objective Value: 0.03234472307443938\n",
            "Iteration number: 18 Current Objective Value: 0.032545912094453185\n",
            "Iteration number: 19 Current Objective Value: 0.034626478322791414\n",
            "Iteration number: 20 Current Objective Value: 0.031447932263402575\n",
            "Iteration number: 21 Current Objective Value: 0.03177918448610578\n",
            "Iteration number: 22 Current Objective Value: 0.030440703102620495\n",
            "Iteration number: 23 Current Objective Value: 0.030366180779198327\n",
            "Iteration number: 24 Current Objective Value: 0.02996622760906693\n",
            "Iteration number: 25 Current Objective Value: 0.02954824972158047\n",
            "Iteration number: 26 Current Objective Value: 0.029288811390686925\n",
            "Iteration number: 27 Current Objective Value: 0.029117186715264826\n",
            "Iteration number: 28 Current Objective Value: 0.028781181816238493\n",
            "Iteration number: 29 Current Objective Value: 0.029003285509934074\n",
            "Iteration number: 30 Current Objective Value: 0.028669928018977157\n",
            "Iteration number: 31 Current Objective Value: 0.028679485018438533\n",
            "Iteration number: 32 Current Objective Value: 0.028509937862328753\n",
            "Iteration number: 33 Current Objective Value: 0.028329275313363645\n",
            "Iteration number: 34 Current Objective Value: 0.0287784221440854\n",
            "Iteration number: 35 Current Objective Value: 0.028034650631746465\n",
            "Iteration number: 36 Current Objective Value: 0.028298146732321627\n",
            "Iteration number: 37 Current Objective Value: 0.028287872995910717\n",
            "Iteration number: 38 Current Objective Value: 0.027968421645816137\n",
            "Iteration number: 39 Current Objective Value: 0.02760640142213103\n",
            "Iteration number: 40 Current Objective Value: 0.02755170990746316\n",
            "Iteration number: 41 Current Objective Value: 0.02751286449816092\n",
            "Iteration number: 42 Current Objective Value: 0.027391633563048447\n",
            "Iteration number: 43 Current Objective Value: 0.027362379326502782\n",
            "Iteration number: 44 Current Objective Value: 0.02761230083115082\n",
            "Iteration number: 45 Current Objective Value: 0.02714563601916238\n",
            "Iteration number: 46 Current Objective Value: 0.027162314822301012\n",
            "Iteration number: 47 Current Objective Value: 0.027365229603224312\n",
            "Iteration number: 48 Current Objective Value: 0.02692443122320446\n",
            "Iteration number: 49 Current Objective Value: 0.027414322595706656\n",
            "Iteration number: 50 Current Objective Value: 0.026958388461626816\n",
            "Iteration number: 51 Current Objective Value: 0.0269412489668746\n",
            "Iteration number: 52 Current Objective Value: 0.026847525594989638\n",
            "Iteration number: 53 Current Objective Value: 0.02707110905261372\n",
            "Iteration number: 54 Current Objective Value: 0.026784323285722573\n",
            "Iteration number: 55 Current Objective Value: 0.02675505387612634\n",
            "Iteration number: 56 Current Objective Value: 0.030668723547836424\n",
            "Iteration number: 57 Current Objective Value: 0.02669771457481314\n",
            "Iteration number: 58 Current Objective Value: 0.026792299442036486\n",
            "Iteration number: 59 Current Objective Value: 0.026902359865693497\n",
            "Iteration number: 60 Current Objective Value: 0.026645766627871006\n",
            "Iteration number: 61 Current Objective Value: 0.02657268898146742\n",
            "Iteration number: 62 Current Objective Value: 0.02675015770090607\n",
            "Iteration number: 63 Current Objective Value: 0.02651679498756633\n",
            "Iteration number: 64 Current Objective Value: 0.026537546504549665\n",
            "Iteration number: 65 Current Objective Value: 0.026480897578194888\n",
            "Iteration number: 66 Current Objective Value: 0.026517776002658283\n",
            "Iteration number: 67 Current Objective Value: 0.02644625199057096\n",
            "Iteration number: 68 Current Objective Value: 0.02669841368681592\n",
            "Iteration number: 69 Current Objective Value: 0.02640934287795669\n",
            "Iteration number: 70 Current Objective Value: 0.02642775465112707\n",
            "Iteration number: 71 Current Objective Value: 0.02637429981461064\n",
            "Iteration number: 72 Current Objective Value: 0.026370130260207823\n",
            "Iteration number: 73 Current Objective Value: 0.026353307080918823\n",
            "Iteration number: 74 Current Objective Value: 0.026713073514442763\n",
            "Iteration number: 75 Current Objective Value: 0.02684668357455652\n",
            "Iteration number: 76 Current Objective Value: 0.026317466766106717\n",
            "Iteration number: 77 Current Objective Value: 0.026311506059626434\n",
            "Iteration number: 78 Current Objective Value: 0.026811544222034273\n",
            "Iteration number: 79 Current Objective Value: 0.02627474822674686\n",
            "Iteration number: 80 Current Objective Value: 0.026306189483792715\n",
            "Iteration number: 81 Current Objective Value: 0.026270707770756657\n",
            "Iteration number: 82 Current Objective Value: 0.02625818153783417\n",
            "Iteration number: 83 Current Objective Value: 0.026285279416311383\n",
            "Iteration number: 84 Current Objective Value: 0.026521007733236197\n",
            "Iteration number: 85 Current Objective Value: 0.030023850239563565\n",
            "Iteration number: 86 Current Objective Value: 0.026230991741370287\n",
            "Iteration number: 87 Current Objective Value: 0.026260229870517997\n",
            "Iteration number: 88 Current Objective Value: 0.026482709654161482\n",
            "Iteration number: 89 Current Objective Value: 0.026205249771224338\n",
            "Iteration number: 90 Current Objective Value: 0.02653964808307713\n",
            "Iteration number: 91 Current Objective Value: 0.026195522756729803\n",
            "Iteration number: 92 Current Objective Value: 0.026195315320358563\n",
            "Iteration number: 93 Current Objective Value: 0.02618814401964884\n",
            "Iteration number: 94 Current Objective Value: 0.026183506034232466\n",
            "Iteration number: 95 Current Objective Value: 0.02617939927421957\n",
            "Iteration number: 96 Current Objective Value: 0.026181254992085172\n",
            "Iteration number: 97 Current Objective Value: 0.026199391805625867\n",
            "Iteration number: 98 Current Objective Value: 0.026684181176969253\n",
            "Iteration number: 99 Current Objective Value: 0.026159659412533735\n",
            "Iteration number: 100 Current Objective Value: 0.026204621331469304\n"
          ]
        }
      ],
      "source": [
        "# Train regularized logistric regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "lam = 0.000001\n",
        "learning_rate = 1\n",
        "w = np.zeros((x_train.shape[1],1))\n",
        "w_mbgd_r, objvals_mbgdr = mbgd(x_train, y_train, lam, learning_rate, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAjw3MyhH5OB"
      },
      "source": [
        "# 4. Compare GD, SGD, MBGD\n",
        "\n",
        "### Plot objective function values against epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {
        "id": "ojqpc-nbH5OB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "outputId": "a140ee46-b977-4145-9dac-86d2c3ea57f2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "max_epoch = 100\n",
        "\n",
        "line0, = plt.plot(range(0,max_epoch), objvals_gd, '-b')\n",
        "line1, = plt.plot(range(0,max_epoch), objvals_sgd, '-r')\n",
        "line2, = plt.plot(range(0,max_epoch), objvals_mbgd, '-g')\n",
        "\n",
        "plt.ylabel('Objective Value', FontSize=10)\n",
        "plt.xlabel('Epochs', FontSize=10)\n",
        "plt.yticks(FontSize=10)\n",
        "plt.xticks(FontSize=10)\n",
        "plt.legend([line0, line1, line2], ['GD', 'SGD', 'mbSGD'], fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "line0, = plt.plot(range(0,max_epoch), objvals_gdr, '-b')\n",
        "line1, = plt.plot(range(0,max_epoch), objvals_sgdr, '-r')\n",
        "line2, = plt.plot(range(0,max_epoch), objvals_mbgdr, '-g')\n",
        "\n",
        "plt.ylabel('Objective Value', FontSize=10)\n",
        "plt.xlabel('Epochs', FontSize=10)\n",
        "plt.yticks(FontSize=10)\n",
        "plt.xticks(FontSize=10)\n",
        "plt.legend([line0, line1, line2], ['GD', 'SGD', 'mbSGD'], fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSLdLqMBH5OB"
      },
      "source": [
        "# 5. Prediction\n",
        "### Compare the training and testing accuracy for logistic regression and regularized logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {
        "id": "PutBwV6rH5OB"
      },
      "outputs": [],
      "source": [
        "# Predict class label\n",
        "# Inputs:\n",
        "#     w: weights: d-by-1 matrix\n",
        "#     X: data: m-by-d matrix\n",
        "# Return:\n",
        "#     f: m-by-1 matrix, the predictions\n",
        "def predict(w, X):\n",
        "  f = np.sign(np.dot(X,w))\n",
        "  return np.asarray(f)\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {
        "id": "hu5O82FyH5OB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f4e1600-5aa7-4346-db96-bebe98e0fb7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classification error for classic Gradient Descent is 0.013186813186813187\n",
            "Training classification error for Stochastic Gradient Descent is 0.006593406593406593\n",
            "Training classification error for Mini-Batch Gradient Descent is 0.008791208791208791\n",
            "Training classification error for classic Gradient Descent regularized is 0.013186813186813187\n",
            "Training classification error for Stochastic Gradient Descent regularized is 0.004395604395604396\n",
            "Training classification error for Mini-Batch Gradient Descent regularized is 0.008791208791208791\n"
          ]
        }
      ],
      "source": [
        "# evaluate training error of logistric regression and regularized version\n",
        "\n",
        "# for gd\n",
        "f_train_gd = predict(w_gd, x_train)\n",
        "diff = numpy.abs(f_train_gd - y_train) / 2\n",
        "error_train_gd = numpy.mean(diff)\n",
        "print('Training classification error for classic Gradient Descent is ' + str(error_train_gd))\n",
        "\n",
        "# for sgd\n",
        "f_train_sgd = predict(w_sgd, x_train)\n",
        "diff = numpy.abs(f_train_sgd - y_train) / 2\n",
        "error_train_sgd = numpy.mean(diff)\n",
        "print('Training classification error for Stochastic Gradient Descent is ' + str(error_train_sgd))\n",
        "\n",
        "# for mini batch gd\n",
        "f_train_mbgd = predict(w_mbgd, x_train)\n",
        "diff = numpy.abs(f_train_mbgd - y_train) / 2\n",
        "error_train_mbgd = numpy.mean(diff)\n",
        "print('Training classification error for Mini-Batch Gradient Descent is ' + str(error_train_mbgd))\n",
        "\n",
        "# for gd reg\n",
        "f_train_gd_r = predict(w_gd_r, x_train)\n",
        "diff = numpy.abs(f_train_gd_r - y_train) / 2\n",
        "error_train_gd_r = numpy.mean(diff)\n",
        "print('Training classification error for classic Gradient Descent regularized is ' + str(error_train_gd_r))\n",
        "\n",
        "# for sgd reg\n",
        "f_train_sgd_r = predict(w_sgd_r, x_train)\n",
        "diff = numpy.abs(f_train_sgd_r - y_train) / 2\n",
        "error_train_sgd_r = numpy.mean(diff)\n",
        "print('Training classification error for Stochastic Gradient Descent regularized is ' + str(error_train_sgd_r))\n",
        "\n",
        "# for mini batch gd reg\n",
        "f_train_mbgd_r = predict(w_mbgd_r, x_train)\n",
        "diff = numpy.abs(f_train_mbgd_r - y_train) / 2\n",
        "error_train_mbgd_r = numpy.mean(diff)\n",
        "print('Training classification error for Mini-Batch Gradient Descent regularized is ' + str(error_train_mbgd_r))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {
        "id": "Nh0r2alfH5OC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "939b225b-410d-4f5c-baf3-6ecf2858ccee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing classification error for classic Gradient Descent is 0.02631578947368421\n",
            "Testing classification error for Stochastic Gradient Descent is 0.05263157894736842\n",
            "Testing classification error for Mini-Batch Gradient Descent is 0.03508771929824561\n",
            "Testing classification error for classic Gradient Descent regularized is 0.02631578947368421\n",
            "Testing classification error for Stochastic Gradient Descent regularized is 0.05263157894736842\n",
            "Testing classification error for Mini-Batch Gradient Descent regularized is 0.03508771929824561\n"
          ]
        }
      ],
      "source": [
        "# evaluate testing error of logistric regression and regularized version\n",
        "\n",
        "# for gd\n",
        "f_test_gd = predict(w_gd, x_test)\n",
        "diff = numpy.abs(f_test_gd - y_test) / 2\n",
        "error_test_gd = numpy.mean(diff)\n",
        "print('Testing classification error for classic Gradient Descent is ' + str(error_test_gd))\n",
        "\n",
        "# for sgd\n",
        "f_test_sgd = predict(w_sgd, x_test)\n",
        "diff = numpy.abs(f_test_sgd - y_test) / 2\n",
        "error_test_sgd = numpy.mean(diff)\n",
        "print('Testing classification error for Stochastic Gradient Descent is ' + str(error_test_sgd))\n",
        "\n",
        "# for mini batch gd\n",
        "f_test_mbgd = predict(w_mbgd, x_test)\n",
        "diff = numpy.abs(f_test_mbgd - y_test) / 2\n",
        "error_test_mbgd = numpy.mean(diff)\n",
        "print('Testing classification error for Mini-Batch Gradient Descent is ' + str(error_test_mbgd))\n",
        "\n",
        "# for gd\n",
        "f_test_gd_r = predict(w_gd_r, x_test)\n",
        "diff = numpy.abs(f_test_gd_r - y_test) / 2\n",
        "error_test_gd_r = numpy.mean(diff)\n",
        "print('Testing classification error for classic Gradient Descent regularized is ' + str(error_test_gd_r))\n",
        "\n",
        "# for sgd reg\n",
        "f_test_sgd_r = predict(w_sgd_r, x_test)\n",
        "diff = numpy.abs(f_test_sgd_r - y_test) / 2\n",
        "error_test_sgd_r = numpy.mean(diff)\n",
        "print('Testing classification error for Stochastic Gradient Descent regularized is ' + str(error_test_sgd_r))\n",
        "\n",
        "# for mini batch gd reg\n",
        "f_test_mbgd_r = predict(w_mbgd_r, x_test)\n",
        "diff = numpy.abs(f_test_mbgd_r - y_test) / 2\n",
        "error_test_mbgd_r = numpy.mean(diff)\n",
        "print('Testing classification error for Mini-Batch Gradient Descent regularized is ' + str(error_test_mbgd_r))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5g_pgSDH5OC"
      },
      "source": [
        "# 6. Parameters tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlfMLldeH5OC"
      },
      "source": [
        "### In this section, you may try different combinations of parameters (regularization value, learning rate, etc) to see their effects on the model. (Open ended question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {
        "id": "RbhMj0FOH5OC"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JSLdLqMBH5OB",
        "m5g_pgSDH5OC",
        "xlfMLldeH5OC"
      ],
      "name": "Assignment 1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}